Warning: P2P NCCL disaggregated prefill XpYd support for vLLM v1 is experimental and subject to change.
Running full benchmark sweep for 8 GPUs.
Constraint: prefill_tp * num_prefill + decode_tp * num_decode = 8
Constraint: prefill_tp >= 4, decode_tp >= 4

HF_TOKEN is set and valid.
Found 8 GPUs.
Checking if pandas is installed...
pandas is installed.
Checking if datasets is installed...
datasets is installed.
Checking if vllm is installed...
vllm is installed.
Checking if quart is installed...
quart is installed.
Starting benchmark sweep...
======================================================================
Running benchmark for config: 1p4tp_1d4tp
  Prefill Servers: 1, Prefill TP: 4
  Decode Servers: 1, Decode TP: 4
======================================================================
Generated Configuration:
  Model: meta-llama/Meta-Llama-3-70B-Instruct
  Prefill Ports: 20003
  Decode Ports: 20005
  Proxy Port: 30001

Starting proxy server on port 30001...
Starting 1 prefill server(s)...
  Prefill server 1: GPUs 0,1,2,3, Port 20003, TP 4

Starting 1 decode server(s)...
  Decode server 1: GPUs 4,5,6,7, Port 20005, TP 4

Waiting for all servers to start...
Waiting for server on port 20003...
Server on port 20003 is ready.
Waiting for server on port 20005...
Server on port 20005 is ready.

All servers are up. Starting benchmark for 1p4tp_1d4tp...
INFO 10-21 22:22:50 [__init__.py:225] Automatically detected platform cuda.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f475c9ca200>, seed=1761085368, num_prompts=256, dataset_name='random', no_stream=False, dataset_path=None, no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=2048, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 0}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=10001, endpoint='/v1/completions', header=None, max_concurrency=None, model='meta-llama/Meta-Llama-3-70B-Instruct', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=10.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=True, save_detailed=True, append_result=False, metadata=None, result_dir=None, result_filename='disagg_P_tp4_dp1_D_tp4_dp1_concurrency64.json', ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='benchmark-serving', top_p=None, top_k=None, min_p=None, temperature=None, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
INFO 10-21 22:22:55 [datasets.py:603] Sampling input_len from [1023, 1023] and output_len from [2048, 2048]
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
 |                                                                                                               | 00:05 elapsed, 982:53:14 remaining
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 10.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: None
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:22<00:00,  1.27it/s]
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     256       
Failed requests:                         0         
Request rate configured (RPS):           10.00     
Benchmark duration (s):                  202.22    
Total input tokens:                      261888    
Total generated tokens:                  261365    
Request throughput (req/s):              1.27      
Output token throughput (tok/s):         1292.45   
Peak output token throughput (tok/s):    2816.00   
Peak concurrent requests:                154.00    
Total Token throughput (tok/s):          2587.49   
---------------Time to First Token----------------
Mean TTFT (ms):                          4371.38   
Median TTFT (ms):                        422.15    
P99 TTFT (ms):                           83763.26  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          64.17     
Median TPOT (ms):                        51.52     
P99 TPOT (ms):                           99.78     
---------------Inter-token Latency----------------
Mean ITL (ms):                           52.40     
Median ITL (ms):                         46.35     
P99 ITL (ms):                            118.50    
==================================================
/home/ubuntu/pdd/vllm/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd
Benchmarking for 1p4tp_1d4tp done. Cleaning up...
./disagg_test.sh: line 273: 435746 Killed                  python3 disagg_proxy_p2p_nccl_xpyd.py > "proxy_${config_name}.log" 2>&1
Cleanup for 1p4tp_1d4tp complete.
All benchmark combinations complete.