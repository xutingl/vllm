{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 14:47:46 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "!export CUDA_VISIBLE_DEVICES=1\n",
    "#from vllm.model_executor.models.llama_ee import LlamaForCausalLMEE\n",
    "#ModelRegistry.register_model(\"LlamaForCausalLMEE\", LlamaForCausalLMEE)\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_inference(llm, input_file, output_file):\n",
    "    prompts = []\n",
    "    with open(input_file, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            prompts.append(json.loads(line)[\"text\"])\n",
    "\n",
    "    sampling_params = SamplingParams(seed=42, max_tokens=100)\n",
    "\n",
    "    start_time = time.time()\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    end_time = time.time()\n",
    "\n",
    "    input_prompt = []\n",
    "    generated_text = []\n",
    "    arrival_time = []\n",
    "    last_token_time = []\n",
    "    first_scheduled_time = []\n",
    "    first_token_time = []\n",
    "    time_in_queue = []\n",
    "    finished_time = []\n",
    "    scheduler_time = []\n",
    "    model_forward_time = []\n",
    "    model_execute_time = []\n",
    "\n",
    "    generated_len = 0 # number of tokens generated\n",
    "    for output in outputs:\n",
    "        input_prompt.append(f\"\\\"{output.prompt!r}\\\"\")\n",
    "        generated_text.append(f\"\\\"{output.outputs[0].text!r}\\\"\")\n",
    "        generated_len += len(output.outputs[0].token_ids)\n",
    "        metrics = output.metrics\n",
    "        arrival_time.append(metrics.arrival_time)\n",
    "        last_token_time.append(metrics.last_token_time)\n",
    "        first_scheduled_time.append(metrics.first_scheduled_time)\n",
    "        first_token_time.append(metrics.first_token_time)\n",
    "        time_in_queue.append(metrics.time_in_queue)\n",
    "        finished_time.append(metrics.finished_time)\n",
    "        scheduler_time.append(metrics.scheduler_time)\n",
    "        model_forward_time.append(metrics.model_forward_time)\n",
    "        model_execute_time.append(metrics.model_execute_time)\n",
    "    \n",
    "    output_throughput = generated_len / (end_time - start_time)\n",
    "    print(f\"Overall output throughput: {output_throughput} tokens/second\")\n",
    "    df = pd.DataFrame({\n",
    "        \"input_prompt\": input_prompt,\n",
    "        \"generated_text\": generated_text,\n",
    "        \"arrival_time\": arrival_time,\n",
    "        \"last_token_time\": last_token_time,\n",
    "        \"first_scheduled_time\": first_scheduled_time,\n",
    "        \"first_token_time\": first_token_time,\n",
    "        \"time_in_queue\": time_in_queue,\n",
    "        \"finished_time\": finished_time,\n",
    "        \"scheduler_time\": scheduler_time,\n",
    "        \"model_forward_time\": model_forward_time,\n",
    "        \"model_execute_time\": model_execute_time,\n",
    "        \"throughput\": [output_throughput] * len(input_prompt)\n",
    "    })\n",
    "    df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 14:48:01 config.py:520] This model supports multiple tasks: {'score', 'classify', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 03-24 14:48:01 cuda.py:100] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 03-24 14:48:01 config.py:656] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 03-24 14:48:01 llm_engine.py:232] Initializing an LLM engine (v0.7.0) with config: model='meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-2-7b-chat-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 03-24 14:48:03 cuda.py:174] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-24 14:48:03 cuda.py:222] Using XFormers backend.\n",
      "INFO 03-24 14:48:04 model_runner.py:1110] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "INFO 03-24 14:48:05 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17d901244db46b390248d69c18f3ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 14:48:10 model_runner.py:1115] Loading model weights took 12.5523 GB\n",
      "INFO 03-24 14:48:11 worker.py:266] Memory profiling takes 0.93 seconds\n",
      "INFO 03-24 14:48:11 worker.py:266] the current vLLM instance can use total_gpu_memory (15.77GiB) x gpu_memory_utilization (0.90) = 14.19GiB\n",
      "INFO 03-24 14:48:11 worker.py:266] model weights take 12.55GiB; non_torch_memory takes 0.07GiB; PyTorch activation peak memory takes 0.67GiB; the rest of the memory reserved for KV Cache is 0.90GiB.\n",
      "INFO 03-24 14:48:11 executor_base.py:108] # CUDA blocks: 115, # CPU blocks: 512\n",
      "INFO 03-24 14:48:11 executor_base.py:113] Maximum concurrency for 1024 tokens per request: 1.80x\n",
      "[CacheEngine._allocate_kv_cache] kv_cache_shape: (2, 115, 65536)\n",
      "[CacheEngine._allocate_kv_cache] kv_cache_shape: (2, 512, 65536)\n",
      "INFO 03-24 14:48:14 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 4.03 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"meta-llama/Llama-2-7b-chat-hf\", max_model_len=1024, enforce_eager=True, max_num_seqs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 14:24:46 config.py:520] This model supports multiple tasks: {'reward', 'score', 'classify', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 03-24 14:24:46 cuda.py:100] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 03-24 14:24:46 config.py:656] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 03-24 14:24:46 llm_engine.py:232] Initializing an LLM engine (v0.7.0) with config: model='meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-2-7b-chat-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 03-24 14:24:48 cuda.py:174] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-24 14:24:48 cuda.py:222] Using XFormers backend.\n",
      "INFO 03-24 14:24:49 model_runner.py:1110] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "INFO 03-24 14:24:49 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12222e83e5fd4a18871ab4168c3430a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 14:24:55 model_runner.py:1115] Loading model weights took 12.5523 GB\n",
      "INFO 03-24 14:24:56 worker.py:266] Memory profiling takes 0.87 seconds\n",
      "INFO 03-24 14:24:56 worker.py:266] the current vLLM instance can use total_gpu_memory (15.77GiB) x gpu_memory_utilization (0.90) = 14.19GiB\n",
      "INFO 03-24 14:24:56 worker.py:266] model weights take 12.55GiB; non_torch_memory takes 0.07GiB; PyTorch activation peak memory takes 0.18GiB; the rest of the memory reserved for KV Cache is 1.39GiB.\n",
      "INFO 03-24 14:24:56 executor_base.py:108] # CUDA blocks: 177, # CPU blocks: 512\n",
      "INFO 03-24 14:24:56 executor_base.py:113] Maximum concurrency for 1024 tokens per request: 2.77x\n",
      "[CacheEngine._allocate_kv_cache] kv_cache_shape: (2, 177, 65536)\n",
      "[CacheEngine._allocate_kv_cache] kv_cache_shape: (2, 512, 65536)\n",
      "INFO 03-24 14:24:59 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 3.91 seconds\n"
     ]
    }
   ],
   "source": [
    "# Below is the output of running the original llama code (llama_original.py) for reference\n",
    "original_llm = LLM(model=\"meta-llama/Llama-2-7b-chat-hf\", max_model_len=1024, enforce_eager=True, max_num_seqs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4/4 [00:00<00:00,  8.39it/s, est. speed input: 56.62 toks/s, output: 134.20 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Hello, my name is', Generated text: \" Sherry and I'm a 35-year-old woman from\"\n",
      "Prompt: 'The president of the United States is', Generated text: ' a member of Congress, which means that he or she is subject to the same'\n",
      "Prompt: 'The capital of France is', Generated text: ' Paris. This is a fact that is well known and widely accepted. However,'\n",
      "Prompt: 'The future of AI is', Generated text: ' likely to be shaped by a combination of technological advancements, soci'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Output of original lammama for reference\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape before converting: torch.Size([27])\n",
      "input_ids shape after converting: torch.Size([1, 27])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 27])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 27])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 2]\n",
      "skip_mask: False, conf: 0.0088043212890625\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 3]\n",
      "skip_mask: False, conf: 0.1258544921875\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 4]\n",
      "skip_mask: False, conf: 0.014495849609375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 5]\n",
      "skip_mask: False, conf: 0.00244140625\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 6]\n",
      "skip_mask: False, conf: 0.84619140625\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 7]\n",
      "skip_mask: False, conf: 0.591796875\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [1, 7]\n",
      "skip_mask: True, conf: 0.9990234375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [1, 8]\n",
      "skip_mask: False, conf: 0.3779296875\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 8]\n",
      "skip_mask: True, conf: 0.98876953125\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 9]\n",
      "skip_mask: False, conf: 0.8046875\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 10]\n",
      "skip_mask: False, conf: 0.2178955078125\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4/4 [00:00<00:00,  4.47it/s, est. speed input: 30.20 toks/s, output: 71.59 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 11]\n",
      "skip_mask: False, conf: 0.210693359375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 12]\n",
      "skip_mask: False, conf: 0.394287109375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 13]\n",
      "skip_mask: False, conf: 0.008636474609375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 14]\n",
      "skip_mask: False, conf: 0.021636962890625\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 15]\n",
      "skip_mask: False, conf: 0.160888671875\n",
      "--------EE statistics---------\n",
      "Prompt: 'Hello, my name is', Generated text: \" Sherry and I'm addicted addicted to reading. I know,\"\n",
      "Prompt: 'The president of the United States is', Generated text: ' a member of Congress, which means that they are an elected official who serves in'\n",
      "Prompt: 'The capital of France is', Generated text: ' Paris. This is a factoid that many people know and can easily verify.'\n",
      "Prompt: 'The future of AI is', Generated text: ' likely to be shaped by factors such as advances in computing power, data'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# output of llama-ee (current llama.py is llama-ee)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([65])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([65])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([65, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([65, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.26s/it, est. speed input: 25.81 toks/s, output: 79.40 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "Overall output throughput: 78.98099678694547 tokens/second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_batch_inference(original_llm, \"lmsys_1.jsonl\", \"results/lmsys_1_original.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape before converting: torch.Size([65])\n",
      "input_ids shape after converting: torch.Size([65])\n",
      "LlamaModel forward. input_ids shape: torch.Size([65])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([65])\n",
      "[compute_logits] hidden_states shape: torch.Size([65, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [30, 172]\n",
      "skip_mask: False, conf: 0.005096435546875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([65, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([65, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([65, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [30, 173]\n",
      "skip_mask: False, conf: 0.1268310546875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [30, 174]\n",
      "skip_mask: False, conf: 0.000274658203125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [30, 175]\n",
      "skip_mask: False, conf: 0.05255126953125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [30, 176]\n",
      "skip_mask: False, conf: 0.00958251953125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [30, 177]\n",
      "skip_mask: False, conf: 0.27099609375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [30, 178]\n",
      "skip_mask: False, conf: 0.00531005859375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [30, 179]\n",
      "skip_mask: False, conf: 0.04541015625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [30, 180]\n",
      "skip_mask: False, conf: 0.03594970703125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [30, 181]\n",
      "skip_mask: False, conf: 0.5966796875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [30, 182]\n",
      "skip_mask: False, conf: 0.546875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [30, 183]\n",
      "skip_mask: False, conf: 0.178466796875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [30, 184]\n",
      "skip_mask: False, conf: 0.01220703125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [30, 185]\n",
      "skip_mask: False, conf: 0.05535888671875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [30, 186]\n",
      "skip_mask: False, conf: 0.065673828125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [30, 187]\n",
      "skip_mask: False, conf: 0.2088623046875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [31, 187]\n",
      "skip_mask: True, conf: 0.95263671875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] SKIPPING!\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [31, 188]\n",
      "skip_mask: False, conf: 0.009765625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [31, 189]\n",
      "skip_mask: False, conf: 0.00634765625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [31, 190]\n",
      "skip_mask: False, conf: 0.0089569091796875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [31, 191]\n",
      "skip_mask: False, conf: 0.0093841552734375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [31, 192]\n",
      "skip_mask: False, conf: 0.003448486328125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [31, 193]\n",
      "skip_mask: False, conf: 0.05810546875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [31, 194]\n",
      "skip_mask: False, conf: 0.369384765625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [32, 194]\n",
      "skip_mask: True, conf: 0.7763671875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] SKIPPING!\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [33, 194]\n",
      "skip_mask: True, conf: 0.95849609375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] SKIPPING!\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [33, 195]\n",
      "skip_mask: False, conf: 0.034423828125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [33, 196]\n",
      "skip_mask: False, conf: 0.03924560546875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [34, 196]\n",
      "skip_mask: True, conf: 0.779296875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] SKIPPING!\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [34, 197]\n",
      "skip_mask: False, conf: 0.00702667236328125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [34, 198]\n",
      "skip_mask: False, conf: 0.1993408203125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [34, 199]\n",
      "skip_mask: False, conf: 0.0011444091796875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [34, 200]\n",
      "skip_mask: False, conf: 0.54345703125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [34, 201]\n",
      "skip_mask: False, conf: 0.357177734375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [34, 202]\n",
      "skip_mask: False, conf: 0.50927734375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [34, 203]\n",
      "skip_mask: False, conf: 0.00146484375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [34, 204]\n",
      "skip_mask: False, conf: 0.10455322265625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [34, 205]\n",
      "skip_mask: False, conf: 0.0090484619140625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [34, 206]\n",
      "skip_mask: False, conf: 0.000980377197265625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [34, 207]\n",
      "skip_mask: False, conf: 0.1507568359375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [34, 208]\n",
      "skip_mask: False, conf: 0.5966796875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [34, 209]\n",
      "skip_mask: False, conf: 0.11968994140625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [34, 210]\n",
      "skip_mask: False, conf: 0.044281005859375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [34, 211]\n",
      "skip_mask: False, conf: 0.0172576904296875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [34, 212]\n",
      "skip_mask: False, conf: 0.022705078125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [35, 212]\n",
      "skip_mask: True, conf: 0.99560546875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] SKIPPING!\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [35, 213]\n",
      "skip_mask: False, conf: 0.01519775390625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [35, 214]\n",
      "skip_mask: False, conf: 0.02899169921875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [35, 215]\n",
      "skip_mask: False, conf: 0.1669921875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [35, 216]\n",
      "skip_mask: False, conf: 0.003692626953125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [35, 217]\n",
      "skip_mask: False, conf: 0.412109375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [35, 218]\n",
      "skip_mask: False, conf: 0.104736328125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [35, 219]\n",
      "skip_mask: False, conf: 0.32861328125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [36, 219]\n",
      "skip_mask: True, conf: 0.82080078125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] SKIPPING!\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [36, 220]\n",
      "skip_mask: False, conf: 0.051361083984375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [36, 221]\n",
      "skip_mask: False, conf: 0.218994140625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [36, 222]\n",
      "skip_mask: False, conf: 0.00585174560546875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [36, 223]\n",
      "skip_mask: False, conf: 0.003688812255859375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [37, 223]\n",
      "skip_mask: True, conf: 0.943359375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] SKIPPING!\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [37, 224]\n",
      "skip_mask: False, conf: 0.345458984375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [37, 225]\n",
      "skip_mask: False, conf: 0.0443115234375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [38, 225]\n",
      "skip_mask: True, conf: 0.99267578125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] SKIPPING!\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [39, 225]\n",
      "skip_mask: True, conf: 0.69580078125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] SKIPPING!\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [39, 226]\n",
      "skip_mask: False, conf: 0.0018310546875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [39, 227]\n",
      "skip_mask: False, conf: 0.348388671875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [39, 228]\n",
      "skip_mask: False, conf: 0.15283203125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [39, 229]\n",
      "skip_mask: False, conf: 0.5751953125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [40, 229]\n",
      "skip_mask: True, conf: 0.8642578125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] SKIPPING!\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [40, 230]\n",
      "skip_mask: False, conf: 0.006927490234375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [41, 230]\n",
      "skip_mask: True, conf: 0.97998046875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] SKIPPING!\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [41, 231]\n",
      "skip_mask: False, conf: 0.000701904296875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [41, 232]\n",
      "skip_mask: False, conf: 0.0149383544921875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [41, 233]\n",
      "skip_mask: False, conf: 0.00016021728515625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [41, 234]\n",
      "skip_mask: False, conf: 0.0684814453125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [41, 235]\n",
      "skip_mask: False, conf: 0.010833740234375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [41, 236]\n",
      "skip_mask: False, conf: 0.0113372802734375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [41, 237]\n",
      "skip_mask: False, conf: 0.0567626953125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [41, 238]\n",
      "skip_mask: False, conf: 0.1416015625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [41, 239]\n",
      "skip_mask: False, conf: 0.004180908203125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [41, 240]\n",
      "skip_mask: False, conf: 0.216064453125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [41, 241]\n",
      "skip_mask: False, conf: 0.00315093994140625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [41, 242]\n",
      "skip_mask: False, conf: 0.01134490966796875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [42, 242]\n",
      "skip_mask: True, conf: 0.99853515625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] SKIPPING!\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [42, 243]\n",
      "skip_mask: False, conf: 0.2041015625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [42, 244]\n",
      "skip_mask: False, conf: 0.003326416015625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [43, 244]\n",
      "skip_mask: True, conf: 0.85693359375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] SKIPPING!\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [43, 245]\n",
      "skip_mask: False, conf: 0.00893402099609375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [43, 246]\n",
      "skip_mask: False, conf: 0.00353240966796875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [43, 247]\n",
      "skip_mask: False, conf: 0.0069427490234375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [43, 248]\n",
      "skip_mask: False, conf: 0.5078125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [43, 249]\n",
      "skip_mask: False, conf: 0.00078582763671875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [44, 249]\n",
      "skip_mask: True, conf: 0.80908203125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] SKIPPING!\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [44, 250]\n",
      "skip_mask: False, conf: 0.000152587890625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [44, 251]\n",
      "skip_mask: False, conf: 0.047119140625\n",
      "--------[LlamaModel: forward] EE statistics---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it, est. speed input: 26.82 toks/s, output: 82.52 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [44, 252]\n",
      "skip_mask: False, conf: 0.0056304931640625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [44, 253]\n",
      "skip_mask: False, conf: 0.0207977294921875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [44, 254]\n",
      "skip_mask: False, conf: 0.453125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [44, 255]\n",
      "skip_mask: False, conf: 0.0308685302734375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [44, 256]\n",
      "skip_mask: False, conf: 0.5986328125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([2])\n",
      "input_ids shape after converting: torch.Size([2])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [44, 257]\n",
      "skip_mask: False, conf: 0.008453369140625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "Overall output throughput: 82.29033866867258 tokens/second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_batch_inference(llm, \"lmsys_1.jsonl\", \"results/lmsys_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-24 12:13:45 scheduler.py:1558] Sequence group 18 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  29%|██▉       | 29/100 [00:10<00:31,  2.29it/s, est. speed input: 254.07 toks/s, output: 264.21 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-24 12:13:57 scheduler.py:1558] Sequence group 38 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  57%|█████▋    | 57/100 [00:22<00:36,  1.17it/s, est. speed input: 278.02 toks/s, output: 238.29 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-24 12:14:08 scheduler.py:1558] Sequence group 72 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:36<00:00,  2.70it/s, est. speed input: 308.53 toks/s, output: 255.23 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall output throughput: 254.72538793728546 tokens/second\n"
     ]
    }
   ],
   "source": [
    "run_batch_inference(llm, \"inputs/lmsys_100.jsonl\", \"results/lmsys_100_eager.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-24 12:11:44 scheduler.py:1558] Sequence group 18 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  41%|████      | 41/100 [00:14<00:19,  2.99it/s, est. speed input: 252.93 toks/s, output: 239.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-24 12:11:59 scheduler.py:1558] Sequence group 48 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  58%|█████▊    | 58/100 [00:25<00:32,  1.29it/s, est. speed input: 255.39 toks/s, output: 197.28 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-24 12:12:10 scheduler.py:1558] Sequence group 75 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:39<00:00,  2.52it/s, est. speed input: 287.37 toks/s, output: 220.44 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall output throughput: 220.03491680150898 tokens/second\n"
     ]
    }
   ],
   "source": [
    "run_batch_inference(llm, \"inputs/lmsys_100.jsonl\", \"results/lmsys_100_lazy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  56%|█████▌    | 56/100 [00:40<00:28,  1.55it/s, est. speed input: 137.51 toks/s, output: 123.71 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-24 14:44:07 scheduler.py:1558] Sequence group 58 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [01:10<00:00,  1.42it/s, est. speed input: 162.51 toks/s, output: 125.95 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall output throughput: 125.82064581870641 tokens/second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# batchsize = 4\n",
    "run_batch_inference(llm, \"inputs/lmsys_100.jsonl\", \"results/lmsys_100_off.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [01:00<00:00,  1.67it/s, est. speed input: 189.99 toks/s, output: 146.85 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall output throughput: 146.67840422611025 tokens/second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# batchsize = 4 + totally turn off\n",
    "run_batch_inference(llm, \"inputs/lmsys_100.jsonl\", \"results/lmsys_100_off.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:33<00:00,  3.00it/s, est. speed input: 342.28 toks/s, output: 264.57 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall output throughput: 264.0085491509796 tokens/second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# batchsize = 8\n",
    "run_batch_inference(original_llm, \"inputs/lmsys_100.jsonl\", \"results/lmsys_100_original.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:58<00:00,  1.71it/s, est. speed input: 195.53 toks/s, output: 151.14 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall output throughput: 150.94671541175452 tokens/second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# batchsize = 4\n",
    "run_batch_inference(original_llm, \"inputs/lmsys_100.jsonl\", \"results/lmsys_100_original.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "megatron_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
