{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 11:43:27 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "!export CUDA_VISIBLE_DEVICES=1\n",
    "#from vllm.model_executor.models.llama_ee import LlamaForCausalLMEE\n",
    "#ModelRegistry.register_model(\"LlamaForCausalLMEE\", LlamaForCausalLMEE)\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_inference(llm, input_file, output_file, batch_size=8):\n",
    "    prompts = []\n",
    "    with open(input_file, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            prompts.append(json.loads(line)[\"text\"])\n",
    "\n",
    "    sampling_params = SamplingParams(seed=42, max_tokens=100)\n",
    "\n",
    "    start_time = time.time()\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    end_time = time.time()\n",
    "\n",
    "    input_prompt = []\n",
    "    generated_text = []\n",
    "    arrival_time = []\n",
    "    last_token_time = []\n",
    "    first_scheduled_time = []\n",
    "    first_token_time = []\n",
    "    time_in_queue = []\n",
    "    finished_time = []\n",
    "    scheduler_time = []\n",
    "    model_forward_time = []\n",
    "    model_execute_time = []\n",
    "\n",
    "    generated_len = 0 # number of tokens generated\n",
    "    for output in outputs:\n",
    "        input_prompt.append(f\"\\\"{output.prompt!r}\\\"\")\n",
    "        generated_text.append(f\"\\\"{output.outputs[0].text!r}\\\"\")\n",
    "        generated_len += len(output.outputs[0].token_ids)\n",
    "        metrics = output.metrics\n",
    "        arrival_time.append(metrics.arrival_time)\n",
    "        last_token_time.append(metrics.last_token_time)\n",
    "        first_scheduled_time.append(metrics.first_scheduled_time)\n",
    "        first_token_time.append(metrics.first_token_time)\n",
    "        time_in_queue.append(metrics.time_in_queue)\n",
    "        finished_time.append(metrics.finished_time)\n",
    "        scheduler_time.append(metrics.scheduler_time)\n",
    "        model_forward_time.append(metrics.model_forward_time)\n",
    "        model_execute_time.append(metrics.model_execute_time)\n",
    "    \n",
    "    output_throughput = generated_len / (end_time - start_time)\n",
    "    print(f\"Overall output throughput: {output_throughput} tokens/second\")\n",
    "    df = pd.DataFrame({\n",
    "        \"input_prompt\": input_prompt,\n",
    "        \"generated_text\": generated_text,\n",
    "        \"arrival_time\": arrival_time,\n",
    "        \"last_token_time\": last_token_time,\n",
    "        \"first_scheduled_time\": first_scheduled_time,\n",
    "        \"first_token_time\": first_token_time,\n",
    "        \"time_in_queue\": time_in_queue,\n",
    "        \"finished_time\": finished_time,\n",
    "        \"scheduler_time\": scheduler_time,\n",
    "        \"model_forward_time\": model_forward_time,\n",
    "        \"model_execute_time\": model_execute_time,\n",
    "        \"throughput\": [output_throughput] * len(input_prompt)\n",
    "    })\n",
    "    df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 11:43:42 config.py:520] This model supports multiple tasks: {'embed', 'generate', 'classify', 'reward', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 03-24 11:43:42 cuda.py:100] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 03-24 11:43:42 config.py:656] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 03-24 11:43:42 llm_engine.py:232] Initializing an LLM engine (v0.7.0) with config: model='meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-2-7b-chat-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 03-24 11:43:44 cuda.py:174] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-24 11:43:44 cuda.py:222] Using XFormers backend.\n",
      "INFO 03-24 11:43:45 model_runner.py:1110] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "INFO 03-24 11:43:46 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0754ef8fe31349e58f2d5068c49726a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 11:43:51 model_runner.py:1115] Loading model weights took 12.5523 GB\n",
      "INFO 03-24 11:43:52 worker.py:266] Memory profiling takes 0.89 seconds\n",
      "INFO 03-24 11:43:52 worker.py:266] the current vLLM instance can use total_gpu_memory (15.77GiB) x gpu_memory_utilization (0.90) = 14.19GiB\n",
      "INFO 03-24 11:43:52 worker.py:266] model weights take 12.55GiB; non_torch_memory takes 0.07GiB; PyTorch activation peak memory takes 0.79GiB; the rest of the memory reserved for KV Cache is 0.78GiB.\n",
      "INFO 03-24 11:43:53 executor_base.py:108] # CUDA blocks: 99, # CPU blocks: 512\n",
      "INFO 03-24 11:43:53 executor_base.py:113] Maximum concurrency for 1024 tokens per request: 1.55x\n",
      "[CacheEngine._allocate_kv_cache] kv_cache_shape: (2, 99, 65536)\n",
      "[CacheEngine._allocate_kv_cache] kv_cache_shape: (2, 512, 65536)\n",
      "INFO 03-24 11:43:55 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 3.99 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"meta-llama/Llama-2-7b-chat-hf\", max_model_len=1024, enforce_eager=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-23 16:34:57 config.py:520] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 03-23 16:34:57 cuda.py:100] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 03-23 16:34:57 config.py:656] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 03-23 16:34:57 llm_engine.py:232] Initializing an LLM engine (v0.7.0) with config: model='meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-2-7b-chat-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 03-23 16:35:00 cuda.py:174] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-23 16:35:00 cuda.py:222] Using XFormers backend.\n",
      "INFO 03-23 16:35:01 model_runner.py:1110] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "Initialized LlamaForCausalLM with prefix \n",
      "INFO 03-23 16:35:01 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6c01ae0e514a39a6720b88b83165fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-23 16:35:06 model_runner.py:1115] Loading model weights took 12.5523 GB\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2048])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2048])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2048, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2048, 4096])\n",
      "INFO 03-23 16:35:07 worker.py:266] Memory profiling takes 0.83 seconds\n",
      "INFO 03-23 16:35:07 worker.py:266] the current vLLM instance can use total_gpu_memory (15.77GiB) x gpu_memory_utilization (0.90) = 14.19GiB\n",
      "INFO 03-23 16:35:07 worker.py:266] model weights take 12.55GiB; non_torch_memory takes 0.07GiB; PyTorch activation peak memory takes 0.31GiB; the rest of the memory reserved for KV Cache is 1.25GiB.\n",
      "INFO 03-23 16:35:07 executor_base.py:108] # CUDA blocks: 160, # CPU blocks: 512\n",
      "INFO 03-23 16:35:07 executor_base.py:113] Maximum concurrency for 1024 tokens per request: 2.50x\n",
      "INFO 03-23 16:35:10 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 3.87 seconds\n"
     ]
    }
   ],
   "source": [
    "# Below is the output of running the original llama code (llama_original.py) for reference\n",
    "original_llm = LLM(model=\"meta-llama/Llama-2-7b-chat-hf\", max_model_len=1024, enforce_eager=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4/4 [00:00<00:00,  8.39it/s, est. speed input: 56.62 toks/s, output: 134.20 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Hello, my name is', Generated text: \" Sherry and I'm a 35-year-old woman from\"\n",
      "Prompt: 'The president of the United States is', Generated text: ' a member of Congress, which means that he or she is subject to the same'\n",
      "Prompt: 'The capital of France is', Generated text: ' Paris. This is a fact that is well known and widely accepted. However,'\n",
      "Prompt: 'The future of AI is', Generated text: ' likely to be shaped by a combination of technological advancements, soci'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Output of original lammama for reference\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape before converting: torch.Size([27])\n",
      "input_ids shape after converting: torch.Size([1, 27])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 27])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 27])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 2]\n",
      "skip_mask: False, conf: 0.0088043212890625\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 3]\n",
      "skip_mask: False, conf: 0.1258544921875\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 4]\n",
      "skip_mask: False, conf: 0.014495849609375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 5]\n",
      "skip_mask: False, conf: 0.00244140625\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 6]\n",
      "skip_mask: False, conf: 0.84619140625\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 7]\n",
      "skip_mask: False, conf: 0.591796875\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [1, 7]\n",
      "skip_mask: True, conf: 0.9990234375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [1, 8]\n",
      "skip_mask: False, conf: 0.3779296875\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 8]\n",
      "skip_mask: True, conf: 0.98876953125\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 9]\n",
      "skip_mask: False, conf: 0.8046875\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 10]\n",
      "skip_mask: False, conf: 0.2178955078125\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4/4 [00:00<00:00,  4.47it/s, est. speed input: 30.20 toks/s, output: 71.59 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 11]\n",
      "skip_mask: False, conf: 0.210693359375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 12]\n",
      "skip_mask: False, conf: 0.394287109375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 13]\n",
      "skip_mask: False, conf: 0.008636474609375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 14]\n",
      "skip_mask: False, conf: 0.021636962890625\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 15]\n",
      "skip_mask: False, conf: 0.160888671875\n",
      "--------EE statistics---------\n",
      "Prompt: 'Hello, my name is', Generated text: \" Sherry and I'm addicted addicted to reading. I know,\"\n",
      "Prompt: 'The president of the United States is', Generated text: ' a member of Congress, which means that they are an elected official who serves in'\n",
      "Prompt: 'The capital of France is', Generated text: ' Paris. This is a factoid that many people know and can easily verify.'\n",
      "Prompt: 'The future of AI is', Generated text: ' likely to be shaped by factors such as advances in computing power, data'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# output of llama-ee (current llama.py is llama-ee)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([65])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([65])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([65, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([65, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.26s/it, est. speed input: 25.81 toks/s, output: 79.40 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2, 4096])\n",
      "Overall output throughput: 78.98099678694547 tokens/second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_batch_inference(original_llm, \"lmsys_1.jsonl\", \"results/lmsys_1_original.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0696, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0013, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0415, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False,  True], device='cuda:0')\n",
      "tensor(0.4463, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([ True, False], device='cuda:0')\n",
      "tensor(0.4592, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.1172, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.1113, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.2150, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.4026, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0922, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.1270, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.3442, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.1499, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([ True, False], device='cuda:0')\n",
      "tensor(0.4590, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([ True, False], device='cuda:0')\n",
      "tensor(0.4927, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0467, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0848, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False,  True], device='cuda:0')\n",
      "tensor(0.4795, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([ True, False], device='cuda:0')\n",
      "tensor(0.3140, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.1918, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([ True, False], device='cuda:0')\n",
      "tensor(0.4763, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0077, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0529, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False,  True], device='cuda:0')\n",
      "tensor(0.3892, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0207, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0023, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False,  True], device='cuda:0')\n",
      "tensor(0.5068, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0442, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.1755, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.1030, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.2228, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0255, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0215, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0064, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False,  True], device='cuda:0')\n",
      "tensor(0.5024, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0229, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.1364, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0033, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0030, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0659, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0833, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0009, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.1929, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0068, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.2441, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0136, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.1042, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0214, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0088, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0036, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([ True, False], device='cuda:0')\n",
      "tensor(0.5791, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0236, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0578, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([ True, False], device='cuda:0')\n",
      "tensor(0.4595, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0080, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0099, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0058, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0018, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0313, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0064, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.1641, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0067, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0224, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.2495, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.1835, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0088, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.1396, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False,  True], device='cuda:0')\n",
      "tensor(0.5098, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0053, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0097, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.1392, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.1450, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.2208, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.1442, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0023, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0049, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False,  True], device='cuda:0')\n",
      "tensor(0.3403, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.1226, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0077, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0391, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0009, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0116, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0357, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.2289, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([ True, False], device='cuda:0')\n",
      "tensor(0.4844, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.1101, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.1672, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False,  True], device='cuda:0')\n",
      "tensor(0.3040, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0050, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it, est. speed input: 26.84 toks/s, output: 82.57 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0158, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([ True, False], device='cuda:0')\n",
      "tensor(0.3352, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([False,  True], device='cuda:0')\n",
      "tensor(0.4626, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.1970, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.2271, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([False,  True], device='cuda:0')\n",
      "tensor(0.5024, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([False,  True], device='cuda:0')\n",
      "tensor(0.3916, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([False,  True], device='cuda:0')\n",
      "tensor(0.4573, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "[get_skip_mask] mask: tensor([False, False], device='cuda:0')\n",
      "tensor(0.0374, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: False\n",
      "[get_skip_mask] mask: tensor([ True, False], device='cuda:0')\n",
      "tensor(0.5083, device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask: True\n",
      "Overall output throughput: 82.38366556777362 tokens/second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_batch_inference(llm, \"lmsys_1.jsonl\", \"results/lmsys_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "megatron_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
