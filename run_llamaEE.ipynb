{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-23 02:29:42 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams, ModelRegistry\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "!export CUDA_VISIBLE_DEVICES=1\n",
    "#from vllm.model_executor.models.llama_ee import LlamaForCausalLMEE\n",
    "#ModelRegistry.register_model(\"LlamaForCausalLMEE\", LlamaForCausalLMEE)\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_inference(llm, input_file, output_file, batch_size=8):\n",
    "    prompts = []\n",
    "    with open(input_file, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            prompts.append(json.loads(line)[\"text\"])\n",
    "\n",
    "    sampling_params = SamplingParams(seed=42, max_tokens=100)\n",
    "\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "    input_prompt = []\n",
    "    generated_text = []\n",
    "    arrival_time = []\n",
    "    last_token_time = []\n",
    "    first_scheduled_time = []\n",
    "    first_token_time = []\n",
    "    time_in_queue = []\n",
    "    finished_time = []\n",
    "    scheduler_time = []\n",
    "    model_forward_time = []\n",
    "    model_execute_time = []\n",
    "    for output in outputs:\n",
    "        input_prompt.append(output.prompt)\n",
    "        generated_text.append(output.outputs[0].text)\n",
    "        metrics = output.metrics\n",
    "        arrival_time.append(metrics.arrival_time)\n",
    "        last_token_time.append(metrics.last_token_time)\n",
    "        first_scheduled_time.append(metrics.first_scheduled_time)\n",
    "        first_token_time.append(metrics.first_token_time)\n",
    "        time_in_queue.append(metrics.time_in_queue)\n",
    "        finished_time.append(metrics.finished_time)\n",
    "        scheduler_time.append(metrics.scheduler_time)\n",
    "        model_forward_time.append(metrics.model_forward_time)\n",
    "        model_execute_time.append(metrics.model_execute_time)\n",
    "    df = pd.DataFrame({\n",
    "        \"input_prompt\": input_prompt,\n",
    "        \"generated_text\": generated_text,\n",
    "        \"arrival_time\": arrival_time,\n",
    "        \"last_token_time\": last_token_time,\n",
    "        \"first_scheduled_time\": first_scheduled_time,\n",
    "        \"first_token_time\": first_token_time,\n",
    "        \"time_in_queue\": time_in_queue,\n",
    "        \"finished_time\": finished_time,\n",
    "        \"scheduler_time\": scheduler_time,\n",
    "        \"model_forward_time\": model_forward_time,\n",
    "        \"model_execute_time\": model_execute_time\n",
    "    })\n",
    "    df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-23 02:29:57 config.py:520] This model supports multiple tasks: {'embed', 'generate', 'reward', 'classify', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 03-23 02:29:57 cuda.py:100] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 03-23 02:29:57 config.py:656] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 03-23 02:29:57 llm_engine.py:232] Initializing an LLM engine (v0.7.0) with config: model='meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-2-7b-chat-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 03-23 02:29:59 cuda.py:174] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-23 02:29:59 cuda.py:222] Using XFormers backend.\n",
      "INFO 03-23 02:30:01 model_runner.py:1110] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "INFO 03-23 02:30:02 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c00f6dca7841e5ac512db703c32c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-23 02:30:07 model_runner.py:1115] Loading model weights took 12.5523 GB\n",
      "input_ids shape before converting: torch.Size([2048])\n",
      "input_ids shape after converting: torch.Size([2048])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2048])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2048])\n",
      "[compute_logits] hidden_states shape: torch.Size([2048, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.4258,  0.3872,  1.4443,  ..., -3.8848,  1.9688, -1.6094],\n",
      "        [ 0.4258,  0.3872,  1.4443,  ..., -3.8848,  1.9688, -1.6094],\n",
      "        [ 0.4258,  0.3872,  1.4443,  ..., -3.8848,  1.9688, -1.6094],\n",
      "        ...,\n",
      "        [ 0.4258,  0.3872,  1.4443,  ..., -3.8848,  1.9688, -1.6094],\n",
      "        [ 0.4258,  0.3872,  1.4443,  ..., -3.8848,  1.9688, -1.6094],\n",
      "        [ 0.4258,  0.3872,  1.4443,  ..., -3.8848,  1.9688, -1.6094]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 1]\n",
      "skip_mask: False, conf: 0.00034332275390625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2048, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2048, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2048, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-3.4492,  5.1875,  3.8145,  ...,  0.0423, -1.7881,  0.1304],\n",
      "        [-3.4492,  5.1875,  3.8145,  ...,  0.0423, -1.7881,  0.1304],\n",
      "        [-3.4492,  5.1875,  3.8145,  ...,  0.0423, -1.7881,  0.1304],\n",
      "        ...,\n",
      "        [-3.4492,  5.1875,  3.8145,  ...,  0.0423, -1.7881,  0.1304],\n",
      "        [-3.4492,  5.1875,  3.8145,  ...,  0.0423, -1.7881,  0.1304],\n",
      "        [-3.4492,  5.1875,  3.8145,  ...,  0.0423, -1.7881,  0.1304]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "INFO 03-23 02:30:08 worker.py:266] Memory profiling takes 0.99 seconds\n",
      "INFO 03-23 02:30:08 worker.py:266] the current vLLM instance can use total_gpu_memory (15.77GiB) x gpu_memory_utilization (0.90) = 14.19GiB\n",
      "INFO 03-23 02:30:08 worker.py:266] model weights take 12.55GiB; non_torch_memory takes 0.07GiB; PyTorch activation peak memory takes 0.79GiB; the rest of the memory reserved for KV Cache is 0.78GiB.\n",
      "INFO 03-23 02:30:08 executor_base.py:108] # CUDA blocks: 99, # CPU blocks: 512\n",
      "INFO 03-23 02:30:08 executor_base.py:113] Maximum concurrency for 1024 tokens per request: 1.55x\n",
      "INFO 03-23 02:30:11 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 4.04 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"meta-llama/Llama-2-7b-chat-hf\", max_model_len=1024, enforce_eager=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-23 00:27:17 config.py:520] This model supports multiple tasks: {'generate', 'reward', 'classify', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 03-23 00:27:17 cuda.py:100] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 03-23 00:27:17 config.py:656] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 03-23 00:27:17 llm_engine.py:232] Initializing an LLM engine (v0.7.0) with config: model='meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-2-7b-chat-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 03-23 00:27:20 cuda.py:174] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-23 00:27:20 cuda.py:222] Using XFormers backend.\n",
      "INFO 03-23 00:27:21 model_runner.py:1110] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "Initialized LlamaForCausalLM with prefix \n",
      "INFO 03-23 00:27:21 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abcaf5d11eac49af87f5f8a38620c620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-23 00:27:26 model_runner.py:1115] Loading model weights took 12.5523 GB\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2048])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2048])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2048, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2048, 4096])\n",
      "[compute_logits] logits: tensor([[-3.4492,  5.1875,  3.8145,  ...,  0.0423, -1.7881,  0.1304],\n",
      "        [-3.4492,  5.1875,  3.8145,  ...,  0.0423, -1.7881,  0.1304],\n",
      "        [-3.4492,  5.1875,  3.8145,  ...,  0.0423, -1.7881,  0.1304],\n",
      "        ...,\n",
      "        [-3.4492,  5.1875,  3.8145,  ...,  0.0423, -1.7881,  0.1304],\n",
      "        [-3.4492,  5.1875,  3.8145,  ...,  0.0423, -1.7881,  0.1304],\n",
      "        [-3.4492,  5.1875,  3.8145,  ...,  0.0423, -1.7881,  0.1304]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "INFO 03-23 00:27:28 worker.py:266] Memory profiling takes 0.97 seconds\n",
      "INFO 03-23 00:27:28 worker.py:266] the current vLLM instance can use total_gpu_memory (15.77GiB) x gpu_memory_utilization (0.90) = 14.19GiB\n",
      "INFO 03-23 00:27:28 worker.py:266] model weights take 12.55GiB; non_torch_memory takes 0.07GiB; PyTorch activation peak memory takes 0.31GiB; the rest of the memory reserved for KV Cache is 1.25GiB.\n",
      "INFO 03-23 00:27:28 executor_base.py:108] # CUDA blocks: 160, # CPU blocks: 512\n",
      "INFO 03-23 00:27:28 executor_base.py:113] Maximum concurrency for 1024 tokens per request: 2.50x\n",
      "INFO 03-23 00:27:30 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 4.02 seconds\n"
     ]
    }
   ],
   "source": [
    "# Below is the output of running the original llama code (llama_original.py) for reference\n",
    "original_llm = LLM(model=\"meta-llama/Llama-2-7b-chat-hf\", max_model_len=1024, enforce_eager=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4/4 [00:00<00:00,  8.39it/s, est. speed input: 56.62 toks/s, output: 134.20 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Hello, my name is', Generated text: \" Sherry and I'm a 35-year-old woman from\"\n",
      "Prompt: 'The president of the United States is', Generated text: ' a member of Congress, which means that he or she is subject to the same'\n",
      "Prompt: 'The capital of France is', Generated text: ' Paris. This is a fact that is well known and widely accepted. However,'\n",
      "Prompt: 'The future of AI is', Generated text: ' likely to be shaped by a combination of technological advancements, soci'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Output of original lammama for reference\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape before converting: torch.Size([27])\n",
      "input_ids shape after converting: torch.Size([1, 27])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 27])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 27])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 2]\n",
      "skip_mask: False, conf: 0.0088043212890625\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 3]\n",
      "skip_mask: False, conf: 0.1258544921875\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 4]\n",
      "skip_mask: False, conf: 0.014495849609375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 5]\n",
      "skip_mask: False, conf: 0.00244140625\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 6]\n",
      "skip_mask: False, conf: 0.84619140625\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 7]\n",
      "skip_mask: False, conf: 0.591796875\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [1, 7]\n",
      "skip_mask: True, conf: 0.9990234375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [1, 8]\n",
      "skip_mask: False, conf: 0.3779296875\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 8]\n",
      "skip_mask: True, conf: 0.98876953125\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 9]\n",
      "skip_mask: False, conf: 0.8046875\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 10]\n",
      "skip_mask: False, conf: 0.2178955078125\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4/4 [00:00<00:00,  4.47it/s, est. speed input: 30.20 toks/s, output: 71.59 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 11]\n",
      "skip_mask: False, conf: 0.210693359375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 12]\n",
      "skip_mask: False, conf: 0.394287109375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 13]\n",
      "skip_mask: False, conf: 0.008636474609375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 14]\n",
      "skip_mask: False, conf: 0.021636962890625\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 15]\n",
      "skip_mask: False, conf: 0.160888671875\n",
      "--------EE statistics---------\n",
      "Prompt: 'Hello, my name is', Generated text: \" Sherry and I'm addicted addicted to reading. I know,\"\n",
      "Prompt: 'The president of the United States is', Generated text: ' a member of Congress, which means that they are an elected official who serves in'\n",
      "Prompt: 'The capital of France is', Generated text: ' Paris. This is a factoid that many people know and can easily verify.'\n",
      "Prompt: 'The future of AI is', Generated text: ' likely to be shaped by factors such as advances in computing power, data'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# output of llama-ee (current llama.py is llama-ee)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([13])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([13])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([13, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([13, 4096])\n",
      "[compute_logits] logits: tensor([[-0.4729,  0.4666, 13.6406,  ...,  0.0463, -0.6255, -1.3252]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-0.2966, -1.0352, 15.0625,  ...,  1.0303,  2.3867,  0.1385]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[ -7.6484, -10.9141,   6.2188,  ...,  -4.0234,  -4.2617,  -1.8525]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-3.7637, -2.5645,  5.1953,  ..., -0.5073, -0.5449,  0.3030]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-3.0977, -3.1875,  6.5273,  ...,  1.2764, -0.8423,  0.2822]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-0.5371, -0.2585, 10.9922,  ...,  1.2432,  0.4355, -0.0755]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-6.1953, -5.6055,  9.1172,  ..., -0.2252, -3.8574, -5.0391]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-1.4697, -0.4517, 11.7500,  ...,  1.4062, -0.3450,  1.1260]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[ 0.3230, -0.5181, 13.0000,  ...,  2.5176,  1.3271,  1.4756]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-1.9434, -3.5527, 12.2109,  ...,  0.5200, -1.6768,  1.1475]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-2.0352, -3.8672, 14.1094,  ..., -0.2927, -3.3145, -1.9131]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-2.9570, -4.5664, 13.5781,  ..., -1.5205, -4.5977, -1.1650]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-2.3477, -3.3047, 13.6406,  ..., -1.5391, -1.0137,  1.1895]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-2.0801, -3.1309, 12.6172,  ..., -0.2542, -2.5879, -2.2656]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-3.7227, -2.6094,  8.1719,  ...,  0.0276, -3.5645, -4.1289]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-2.8340, -2.2148, 11.8984,  ...,  0.0640, -3.4941, -2.5117]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-0.9492, -1.2197, 15.2578,  ...,  2.1641, -1.5801, -2.3496]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.4609, -2.9512, 11.7031,  ..., -1.7256, -3.5469, -4.3867]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-3.3301, -5.1992, 11.0859,  ..., -1.4746, -4.1328, -3.5801]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-3.8066, -2.7031, 11.5234,  ..., -2.7012, -4.3477, -1.8896]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.0625, -1.6904, 12.4844,  ..., -0.4578, -1.8135, -1.7842]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[ 1.0957, -0.7925, 14.6484,  ...,  3.0898,  0.4917, -0.1199]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[ 0.8081, -0.7295, 15.0000,  ...,  0.3704,  0.6035, -0.2708]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-3.6270, -5.9297,  7.0312,  ..., -1.7217, -2.8262, -0.9326]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-1.6309, -1.6396,  7.9297,  ...,  1.0801, -0.2849,  0.4023]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-3.8809, -1.1650,  8.0703,  ..., -1.1113, -2.5996, -3.4883]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.5117, -7.9961,  6.4219,  ..., -1.3184, -3.5469, -1.2451]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-3.5254, -2.6211,  8.9844,  ...,  1.0049, -2.1602,  2.2656]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-2.0410, -4.9258,  9.2734,  ...,  1.0332, -3.2930,  3.2598]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.9961, -6.4062,  4.7148,  ..., -0.4119, -3.8418, -0.7251]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-1.7021, -3.6387, 11.0312,  ...,  1.7207, -2.0137,  2.6758]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-2.4688, -2.0938,  6.9062,  ..., -1.0908, -4.8398,  5.4258]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-3.9941, -4.0781,  5.7227,  ..., -1.2607, -6.5664, -0.6523]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-3.0742,  1.3037,  7.3398,  ..., -1.1133, -4.3828, -2.8926]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-2.7344, -1.3301,  6.4609,  ...,  0.0251, -3.7949, -1.9424]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-2.6992, -1.2471,  9.4609,  ...,  0.6357, -1.3408, -0.6313]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-2.9473, -2.2461, 11.4531,  ...,  0.5552, -2.4414, -0.5088]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.2578, -6.4648,  4.0781,  ..., -2.7617, -5.1562, -2.1133]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.4688,  0.2949,  4.7070,  ..., -0.6084, -6.1211, -4.5820]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-5.6641, -4.7461,  7.9375,  ..., -4.0039, -5.2812, -5.6133]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.6875, -1.6113, 10.0625,  ..., -2.0684, -3.9551, -2.4434]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-3.9238, -3.5176,  9.8672,  ..., -0.3228, -4.1953, -2.5137]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-5.0195, -6.6992, 10.8203,  ...,  0.9077, -2.3379,  1.0410]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-0.2864,  0.6411, 11.1641,  ..., -0.2076, -1.1582,  1.0127]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-3.5977, -5.1133,  8.2578,  ..., -0.3984, -3.4570,  0.4858]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-2.9980, -2.8730,  8.4453,  ..., -1.2344, -3.4414,  0.6206]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-1.7832, -0.4634, 11.8672,  ...,  0.8120, -3.1348,  1.4170]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.7070, -6.0469,  7.0859,  ..., -1.7764, -4.9375, -0.5132]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-1.0869,  2.9043,  9.6875,  ...,  1.2520, -1.8369, -0.5938]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.3086, -3.0488, 11.6172,  ..., -1.1113, -3.9727,  0.0665]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-3.4941, -4.3477, 12.7500,  ...,  0.1764, -3.5762,  0.9961]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-2.2500, -5.1445, 10.9453,  ..., -3.9512, -2.7578,  4.2930]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-2.1953, -1.0049, 11.7031,  ..., -1.8711, -4.5859,  0.5020]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[ 0.0643,  0.6499, 15.2422,  ...,  2.0586, -2.2852,  0.1476]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.0117, -5.8516,  7.1758,  ..., -0.8594, -5.4688, -2.1328]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-2.8359, -5.2969,  5.0703,  ..., -1.6807, -5.3945, -4.5508]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-3.9121, -6.0312,  3.4961,  ..., -4.8359, -3.9102, -3.7500]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.6367, -5.8008,  3.2539,  ..., -4.2227, -5.1680, -3.2617]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.8320, -7.8477,  1.7676,  ..., -3.1680, -6.1055, -2.2500]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.1797, -1.8828,  7.9883,  ..., -2.5996, -3.3223, -1.3652]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[ 0.6636,  0.5947, 12.5859,  ...,  2.6270, -1.7725,  1.4502]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.1953, -6.8125,  9.4453,  ...,  0.9150, -3.7461,  0.5576]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-1.9365, -1.5205, 10.0156,  ...,  0.6387, -2.2363, -1.0068]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-3.7715, -3.3047, 10.8047,  ..., -1.0508, -2.9141, -0.1240]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.1562, -7.5703,  6.2578,  ...,  0.7534, -3.2148,  0.0452]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.4375, -6.4570,  6.2461,  ..., -0.8647, -1.9502, -1.3916]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.3594, -5.7812,  2.8984,  ..., -1.7500, -5.2227, -5.2617]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.5000, -7.1562,  3.5762,  ..., -3.0645, -4.9609, -3.8789]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-5.0430, -3.5566,  5.9531,  ..., -2.1348, -3.1055, -3.8320]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.9883, -4.1914,  6.6953,  ..., -1.7441, -3.9082, -4.9961]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.3203, -7.0391,  1.3438,  ..., -2.8340, -6.6797, -3.5938]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-3.5723,  2.5781, 10.7656,  ..., -1.4961, -4.7266,  2.2051]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.4922, -2.5176,  6.2734,  ..., -0.6338, -4.6484,  0.0153]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.9062, -5.3672,  4.6484,  ..., -0.9307, -5.3164, -3.5898]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-5.1289, -4.8828,  3.7285,  ..., -1.2236, -2.9277, -3.9336]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-5.5078, -4.5039,  3.8652,  ..., -3.5586, -5.3672, -3.7715]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.6016, -1.3145,  7.1445,  ..., -2.1172, -6.1406, -4.7734]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.8359, -5.9219,  8.2422,  ..., -2.7559, -5.3984, -2.5820]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.5938, -2.9922,  2.6797,  ..., -1.7080, -2.4355, -3.8086]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-2.5137, -3.5098,  5.2344,  ..., -1.9102, -1.8809, -1.9854]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-5.3203, -5.5938,  5.9883,  ..., -0.3762, -4.8086, -2.6152]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-6.3281, -6.7422,  8.4844,  ..., -2.5000, -3.1133, -3.5645]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-1.0908,  0.4211, 13.2578,  ...,  3.0391, -0.6714,  1.8164]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-5.0156, -9.1016,  4.1641,  ..., -2.6191, -4.7539,  0.7246]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-3.2031, -3.9062,  5.1172,  ..., -2.8633, -3.2129,  3.9727]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-3.6270, -0.6172,  5.8086,  ..., -1.5146, -4.5117,  0.3904]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.1367, -4.3086,  4.8867,  ..., -1.7637, -4.3906, -2.7578]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.4102, -2.9297,  3.9609,  ..., -2.1328, -7.3281, -2.4512]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-6.5781, -3.6914,  7.1367,  ..., -5.4805, -6.8164, -6.5000]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-5.7188, -2.2520,  8.4297,  ..., -4.0469, -3.7285, -5.0742]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-6.5664, -7.6758,  6.7305,  ..., -3.2852, -6.6602, -1.6045]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-5.1602, -4.1797,  8.9297,  ..., -1.7900, -5.7305,  0.5093]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.0820, -5.2969,  4.9219,  ...,  0.3540, -4.9141,  1.7949]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.55s/it, est. speed input: 5.10 toks/s, output: 39.22 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-4.2891, -4.1367,  4.3320,  ..., -1.7441, -4.8203, -3.4629]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-5.7617, -8.2656,  5.6133,  ..., -2.9238, -1.3457, -3.2051]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[-3.6895, -9.6406,  4.3672,  ..., -0.9741, -2.7520, -3.0293]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[ -4.7578, -13.0000,   3.6250,  ...,  -2.4941,  -3.0977,  -1.4883]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[ -6.3125, -12.4766,   7.8086,  ...,  -3.8301,  -6.2695,  -3.7422]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[ 0.8960,  2.1797, 12.5234,  ...,  0.5415, -0.9253,  0.3242]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[compute_logits] logits: tensor([[ 0.4944,  1.2910, 15.8125,  ...,  3.2246, -0.1403,  0.5513]],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_batch_inference(original_llm, \"lmsys_1.jsonl\", \"results/lmsys_1_original.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape before converting: torch.Size([13])\n",
      "input_ids shape after converting: torch.Size([13])\n",
      "LlamaModel forward. input_ids shape: torch.Size([13])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([13])\n",
      "[compute_logits] hidden_states shape: torch.Size([13, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 4.9219,  2.2461, -4.3906,  ...,  3.3125,  2.5020,  2.0527],\n",
      "        [ 0.1512,  1.1182,  1.6289,  ...,  2.8145,  0.8994, -3.0039],\n",
      "        [-0.6582,  0.8691,  1.9893,  ...,  1.6553, -0.4167, -2.3047],\n",
      "        ...,\n",
      "        [-0.9692, -0.4407, -1.8320,  ...,  0.3606,  1.0049, -0.4836],\n",
      "        [ 1.3262,  6.9922,  0.4502,  ..., -0.4648,  1.8838,  1.0205],\n",
      "        [-1.6396, -1.3789,  3.7227,  ...,  0.9658, -1.4277, -3.4766]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 2]\n",
      "skip_mask: False, conf: 0.005096435546875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([13, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([13, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([13, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.4729,  0.4666, 13.6406,  ...,  0.0463, -0.6255, -1.3252]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.2942, -2.6523,  2.4258,  ...,  0.6372,  0.5913,  0.0958]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 3]\n",
      "skip_mask: False, conf: 0.126953125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.2966, -1.0352, 15.0625,  ...,  1.0303,  2.3867,  0.1385]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 4.9922,  2.2734, -4.4023,  ...,  3.3848,  2.4980,  2.0977]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 4]\n",
      "skip_mask: False, conf: 0.000274658203125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ -7.6484, -10.9141,   6.2188,  ...,  -4.0234,  -4.2617,  -1.8525]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.8135,  0.3606, -0.4241,  ..., -2.6445,  1.2041,  0.5010]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 5]\n",
      "skip_mask: False, conf: 0.05255126953125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-3.7637, -2.5645,  5.1953,  ..., -0.5073, -0.5449,  0.3030]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.5181, -0.9653, -0.9487,  ...,  0.5771,  0.4058,  0.2142]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 6]\n",
      "skip_mask: False, conf: 0.0096435546875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-3.0977, -3.1875,  6.5273,  ...,  1.2764, -0.8423,  0.2822]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.7393, -1.1562, -2.6660,  ..., -1.8613, -0.9917, -3.3809]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 7]\n",
      "skip_mask: False, conf: 0.271484375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.5371, -0.2585, 10.9922,  ...,  1.2432,  0.4355, -0.0755]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.3733,  1.6416, -0.7134,  ..., -2.8184,  1.0352, -3.3359]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 8]\n",
      "skip_mask: False, conf: 0.00555419921875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-6.1953, -5.6055,  9.1172,  ..., -0.2252, -3.8574, -5.0391]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.4336, -1.8926,  0.2859,  ...,  0.0254, -3.1582, -3.6445]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 9]\n",
      "skip_mask: False, conf: 0.04510498046875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.4697, -0.4517, 11.7500,  ...,  1.4062, -0.3450,  1.1260]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.9072, -2.9219,  2.0781,  ...,  0.6157,  2.7344, -2.6602]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 10]\n",
      "skip_mask: False, conf: 0.03582763671875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.3230, -0.5181, 13.0000,  ...,  2.5176,  1.3271,  1.4756]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.9165,  2.8555, -0.3784,  ...,  0.8198, -1.6094,  1.4033]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 11]\n",
      "skip_mask: False, conf: 0.59765625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.9434, -3.5527, 12.2109,  ...,  0.5200, -1.6768,  1.1475]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.1700,  0.6431, -1.3223,  ...,  1.3135, -3.6914, -2.4668]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 12]\n",
      "skip_mask: False, conf: 0.5517578125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.0352, -3.8672, 14.1094,  ..., -0.2927, -3.3145, -1.9131]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.4185,  2.5879, -2.5254,  ..., -0.0098, -0.6919, -0.5898]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 13]\n",
      "skip_mask: False, conf: 0.17822265625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.9570, -4.5664, 13.5781,  ..., -1.5205, -4.5977, -1.1650]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.9126,  1.5996, -4.0156,  ...,  1.0850,  0.5171,  1.6533]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 14]\n",
      "skip_mask: False, conf: 0.0119781494140625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.3477, -3.3047, 13.6406,  ..., -1.5391, -1.0137,  1.1895]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.2539, -2.3066, -0.4143,  ...,  1.2598,  0.2664, -2.1152]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 15]\n",
      "skip_mask: False, conf: 0.05560302734375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.0801, -3.1309, 12.6172,  ..., -0.2542, -2.5879, -2.2656]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.2573,  3.3867, -0.3672,  ...,  1.3320, -1.7539, -1.3818]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 16]\n",
      "skip_mask: False, conf: 0.065673828125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-3.7227, -2.6094,  8.1719,  ...,  0.0276, -3.5645, -4.1289]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.6680,  3.5117, -0.9463,  ...,  2.3574, -0.2485, -1.3223]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 17]\n",
      "skip_mask: False, conf: 0.210693359375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.8340, -2.2148, 11.8984,  ...,  0.0640, -3.4941, -2.5117]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.0605,  4.2148,  1.8281,  ...,  3.2070, -0.6016, -2.1250]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [1, 17]\n",
      "skip_mask: True, conf: 0.95263671875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] SKIPPING!\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.7617, -0.9038,  0.4287,  ...,  2.4785, -2.1387, -5.0586]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.8301,  1.1592, -2.0488,  ..., -1.0674, -3.1758, -0.8921]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [1, 18]\n",
      "skip_mask: False, conf: 0.009765625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.7886, -1.2939, 11.5547,  ...,  1.2061, -2.1855, -0.6064]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.0078, -1.8936, -1.3818,  ..., -1.3730, -0.2700, -0.6011]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [1, 19]\n",
      "skip_mask: False, conf: 0.0052490234375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.1699, -0.6108, 12.4375,  ..., -1.2617, -3.0273, -0.1876]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.1780,  6.1602,  2.7129,  ...,  0.8179, -1.5254,  0.0754]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [1, 20]\n",
      "skip_mask: False, conf: 0.009246826171875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-3.8203, -1.9082, 11.3203,  ...,  0.3059, -2.6777, -1.0215]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.8364, -0.0919,  3.8555,  ...,  2.7324,  2.0391, -2.5156]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [1, 21]\n",
      "skip_mask: False, conf: 0.0089569091796875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 1.5850, -0.2766, 14.6719,  ...,  2.9102,  0.6343,  0.4111]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.9966,  1.4795,  1.2646,  ..., -0.5898, -1.4492,  0.8467]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [1, 22]\n",
      "skip_mask: False, conf: 0.002960205078125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.9141, -0.8452, 14.7266,  ...,  0.6577,  0.9443,  0.1621]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 1.4688,  4.1680, -0.7085,  ..., -0.1512, -0.2129, -0.2927]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [1, 23]\n",
      "skip_mask: False, conf: 0.058135986328125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-3.5977, -5.9102,  7.0664,  ..., -1.5400, -2.5977, -0.7026]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.1865,  1.1787, -0.7104,  ..., -0.7505,  2.5215, -1.5908]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [1, 24]\n",
      "skip_mask: False, conf: 0.369384765625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.3408, -1.5166,  8.1406,  ...,  1.4795,  0.0494,  0.7959]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.2832, -1.2422, -0.2637,  ...,  0.3928,  1.0947, -3.3164]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [1, 25]\n",
      "skip_mask: False, conf: 0.77490234375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.5684, -1.1143, 10.2188,  ...,  0.9966, -1.3730,  1.6416]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.8184,  0.8838,  0.7070,  ..., -1.3193, -2.1895, -2.0352]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [2, 25]\n",
      "skip_mask: True, conf: 0.95947265625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] SKIPPING!\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.0756,  0.1592, -1.1221,  ...,  1.1582, -1.9971,  0.9624]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.8301,  2.7500, -2.1445,  ...,  0.9971, -3.2793,  2.0625]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [2, 26]\n",
      "skip_mask: False, conf: 0.03448486328125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.6016,  0.2842,  8.7422,  ...,  3.8848, -0.6885,  0.5581]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[3.2109, 3.2969, 0.3960,  ..., 1.8330, 0.2479, 1.9121]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [2, 27]\n",
      "skip_mask: False, conf: 0.0384521484375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-3.1914, -2.7109,  4.0430,  ..., -0.4504, -3.3965,  3.5645]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.1442, -0.2399, -0.5752,  ..., -0.2900, -3.2930, -2.4238]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [2, 28]\n",
      "skip_mask: False, conf: 0.7783203125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.7354, -0.1785,  7.0977,  ...,  0.7759, -3.0879, -0.3325]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.5454, -1.2168, -1.4121,  ...,  0.6445, -2.7207, -1.8672]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [2, 29]\n",
      "skip_mask: False, conf: 0.2337646484375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-3.1523, -5.3398,  2.5605,  ..., -2.9805, -4.4844,  0.0223]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.2327,  0.5981, -1.8584,  ...,  3.6660, -3.4082, -1.5850]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [2, 30]\n",
      "skip_mask: False, conf: 0.80810546875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[1.3604, 1.0566, 9.7734,  ..., 1.6973, 1.4141, 1.7900]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.8467, -0.2443, -0.7998,  ...,  2.2949, -2.6660, -2.6641]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [2, 31]\n",
      "skip_mask: False, conf: 0.3935546875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.7432, -7.0000,  8.9609,  ...,  1.1162, -2.1055,  1.1582]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.3105, -2.3672, -0.0833,  ...,  1.0820, -1.4395, -4.0938]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [2, 32]\n",
      "skip_mask: False, conf: 0.212890625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-4.6992, -8.0312,  2.1367,  ..., -2.8320, -7.5078, -2.2168]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.9502, -2.0254,  0.2328,  ..., -1.0898, -0.5752, -0.7837]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [2, 33]\n",
      "skip_mask: False, conf: 0.00400543212890625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-3.8301, -4.7578,  5.5938,  ..., -2.0938, -0.3696, -1.4521]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 2.2480,  5.7812, -0.8140,  ...,  1.6064,  0.9346,  0.2205]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [2, 34]\n",
      "skip_mask: False, conf: 0.01279449462890625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ -6.5234, -16.1875,   2.4629,  ...,  -2.6152,  -2.2070,  -6.3789]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.0205, -0.8232, -1.0010,  ..., -0.6865, -2.2715, -1.5938]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [2, 35]\n",
      "skip_mask: False, conf: 0.310546875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.1797, -0.4858,  6.6289,  ...,  1.4189, -3.0703, -2.6660]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.0664, -0.4583,  0.0715,  ...,  0.2018, -2.5488, -1.8828]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [2, 36]\n",
      "skip_mask: False, conf: 0.072265625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-3.8789, -4.1953,  4.2812,  ...,  0.8540, -0.9795, -2.3320]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 1.0078,  2.7324, -1.7568,  ..., -1.1738,  2.9668,  2.8750]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [2, 37]\n",
      "skip_mask: False, conf: 0.002410888671875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.8955, -0.6270,  9.8750,  ...,  2.8887, -4.5430,  1.4746]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.7319,  2.3125, -1.3750,  ..., -0.2371, -1.5840, -0.5127]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [2, 38]\n",
      "skip_mask: False, conf: 0.37548828125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.9761,  1.3604,  7.3984,  ...,  1.3145, -1.7480, -2.1270]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.1016, -2.2559, -1.7549,  ..., -0.1941,  0.1287,  0.4207]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [2, 39]\n",
      "skip_mask: False, conf: 0.138916015625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.2227, -1.6592,  8.3594,  ..., -0.4265, -2.0488, -2.4316]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.3215,  0.9653, -0.8384,  ...,  1.9590,  2.0117, -2.0332]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [2, 40]\n",
      "skip_mask: False, conf: 0.00982666015625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.9941, -0.7134,  7.4258,  ..., -1.4277, -2.4297, -2.0371]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.8521, -0.3650,  1.6963,  ...,  1.8330,  0.3928,  0.5425]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [2, 41]\n",
      "skip_mask: False, conf: 0.2230224609375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.6016, -2.9023, 10.6484,  ..., -0.1782, -2.5391, -0.2893]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 2.2070,  2.9609, -0.4722,  ..., -0.8438,  0.1558, -0.8584]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [2, 42]\n",
      "skip_mask: False, conf: 0.004638671875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-5.2695, -8.5625,  4.6094,  ..., -0.6655, -6.9414, -2.4551]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.0173, -1.9551, -0.7700,  ..., -1.6846, -1.5996, -0.1501]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 42]\n",
      "skip_mask: True, conf: 0.91015625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] SKIPPING!\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.6943,  1.0439, -0.2935,  ...,  1.0195, -0.0654, -0.5781]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.2310, -1.9199,  0.6914,  ..., -0.4253, -1.5938, -0.6416]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 43]\n",
      "skip_mask: False, conf: 0.00450897216796875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-8.0859, -6.9375,  3.1934,  ..., -3.6816, -7.5977, -3.7188]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[1.5820, 0.7095, 1.7803,  ..., 2.4199, 1.3896, 0.1418]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 44]\n",
      "skip_mask: False, conf: 0.03155517578125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-6.3320, -6.5078,  3.9102,  ..., -3.8418, -5.9961, -2.6445]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.4263, -0.6621, -1.1914,  ...,  3.1953,  0.3982, -0.1602]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 45]\n",
      "skip_mask: False, conf: 0.00975799560546875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-6.4336, -5.4922,  3.3379,  ..., -5.0156, -4.9766, -3.6270]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.2646, -1.2529, -0.0222,  ...,  0.6743,  1.6816, -2.0430]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 46]\n",
      "skip_mask: False, conf: 0.00081634521484375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-4.2891, -3.5371,  4.0664,  ..., -0.5054, -2.9453, -2.4883]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.2637, -4.3867,  0.1534,  ..., -3.1211,  3.5840, -1.3633]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 47]\n",
      "skip_mask: False, conf: 0.174072265625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-3.5762, -5.6602,  5.4492,  ..., -1.2969, -4.1016, -1.6123]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.0745, -0.4875,  0.6426,  ..., -2.2324,  3.1758,  0.5493]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 48]\n",
      "skip_mask: False, conf: 0.019775390625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-5.8320, -7.3555,  4.7539,  ..., -0.3154, -3.7500, -2.1328]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.1445, -1.3213, -1.7129,  ..., -2.7715, -0.4531, -1.2207]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 49]\n",
      "skip_mask: False, conf: 0.135498046875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.2937,  0.3589, 10.6875,  ...,  3.0137,  0.2217,  1.8125]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.2194,  1.9277, -0.9927,  ..., -2.2812, -0.5591, -1.5840]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 50]\n",
      "skip_mask: False, conf: 0.002471923828125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-5.1016, -3.6797,  6.9570,  ..., -0.1183, -1.2988, -3.6484]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.2354, -0.7275, -1.6904,  ...,  1.8525,  0.2886, -1.0947]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 51]\n",
      "skip_mask: False, conf: 0.000110626220703125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-5.0898, -5.8477,  5.6094,  ..., -0.6064, -2.3125, -0.2042]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.7549, -4.8867, -1.1484,  ...,  4.9961,  2.3184, -2.2637]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 52]\n",
      "skip_mask: False, conf: 0.00199127197265625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-7.5703, -9.0391,  4.2383,  ..., -3.1445, -5.6328, -3.5410]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.0420, -4.2500,  0.7393,  ...,  2.1289, -1.8525,  0.0891]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 53]\n",
      "skip_mask: False, conf: 0.0673828125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-4.4805, -6.3594,  7.8164,  ..., -1.7441, -2.0918, -2.4824]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.3750, -4.1875, -0.9727,  ...,  1.3037, -1.6309,  2.2949]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 54]\n",
      "skip_mask: False, conf: 0.03741455078125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-3.4219,  1.4639,  5.9492,  ..., -3.7227, -0.6992, -3.7578]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.9976, -1.3916,  0.3120,  ..., -0.3528,  2.7090, -1.2842]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 55]\n",
      "skip_mask: False, conf: 0.0084686279296875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.6714,  0.6943,  9.2344,  ...,  2.9414,  0.9038,  0.5347]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.4045, -0.0325, -2.1973,  ..., -2.1484,  2.0918, -1.4941]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 56]\n",
      "skip_mask: False, conf: 0.00335693359375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-5.4219, -3.1602,  7.6445,  ...,  0.2671, -1.2861, -3.4785]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.1918,  2.5371, -0.7925,  ...,  2.0938, -0.1536, -0.3936]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 57]\n",
      "skip_mask: False, conf: 0.00930023193359375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-3.7305, -3.5742,  7.3438,  ...,  2.5273, -0.9272,  2.2012]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.1588, -0.6626, -2.1113,  ...,  4.0039, -2.6699,  0.0082]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 58]\n",
      "skip_mask: False, conf: 0.00042724609375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ -7.9570, -11.0156,   3.4629,  ...,  -4.1641,  -6.0078,  -3.1074]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 2.4375,  0.7896,  0.1880,  ..., -2.3320,  0.1884, -0.8008]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 59]\n",
      "skip_mask: False, conf: 0.89794921875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-7.4062, -7.5938,  3.3672,  ..., -3.2246, -8.1797, -2.5156]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.6279, -1.5400,  0.5469,  ...,  2.2227, -3.2480, -0.7251]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 60]\n",
      "skip_mask: False, conf: 0.0067291259765625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-3.4004, -0.0454,  2.9062,  ..., -1.5127, -0.3772, -1.4648]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.5972, -0.8906,  1.5381,  ...,  3.5156, -1.0752, -2.5293]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 61]\n",
      "skip_mask: False, conf: 0.3818359375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-4.8516, -3.0469,  2.6328,  ..., -2.9043, -5.3125, -4.1562]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.1245, -2.5547, -0.5679,  ...,  3.7148, -0.9937, -2.0098]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 62]\n",
      "skip_mask: False, conf: 0.01397705078125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-8.5000, -6.7539,  1.8535,  ..., -3.9434, -7.0977, -4.0234]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.7222, -1.7900, -0.3259,  ...,  3.8613,  1.0244, -3.3574]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 63]\n",
      "skip_mask: False, conf: 0.036651611328125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ -9.6328, -11.2812,   0.6523,  ...,  -3.3203,  -8.9453,  -5.4531]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 1.7236,  6.3516,  1.1123,  ...,  0.6152, -0.8350,  0.5576]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 64]\n",
      "skip_mask: False, conf: 0.002838134765625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-5.9414, -3.5977,  2.8984,  ..., -1.7285, -4.5547,  0.2537]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.7070, -0.5801,  3.0586,  ...,  4.1797,  0.3081,  0.5552]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 65]\n",
      "skip_mask: False, conf: 0.0285186767578125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-7.6172, -7.7109,  4.7812,  ..., -1.3008, -5.4961, -2.3848]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.2271,  1.4268,  0.7153,  ..., -0.3196, -1.1309,  1.0059]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 66]\n",
      "skip_mask: False, conf: 0.5673828125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-5.3359, -3.4082,  5.6328,  ...,  0.6479, -2.7617,  3.8047]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.8682, -3.3574, -0.4529,  ..., -0.7524,  0.7778, -0.3682]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 67]\n",
      "skip_mask: False, conf: 0.035400390625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-5.3281, -7.1133,  1.3975,  ..., -4.9688, -2.3477, -5.0117]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.1584, -1.3887, -1.3096,  ..., -2.1035, -0.9214, -1.5273]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 68]\n",
      "skip_mask: False, conf: 0.045654296875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.2891, -6.7891,  1.3125,  ..., -0.7271, -5.4102, -3.1387]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.4927,  1.1924, -0.8896,  ..., -0.0363, -0.7065, -2.2051]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 69]\n",
      "skip_mask: False, conf: 0.1162109375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-3.7637, -6.0273,  3.3262,  ..., -1.7314, -5.8828, -2.2891]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 1.1357, -2.6367, -0.2532,  ...,  0.3669, -2.6621,  0.1284]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 70]\n",
      "skip_mask: False, conf: 0.0085906982421875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-4.2188, -4.9805,  2.6523,  ..., -3.1152, -5.0586, -1.1328]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 1.1553, -3.4746, -0.5444,  ..., -0.4771, -2.3945, -0.8296]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 71]\n",
      "skip_mask: False, conf: 0.000263214111328125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ -9.4062, -11.3203,   1.5713,  ...,  -6.2852,  -9.3516,  -4.0195]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.0566, -1.9160,  0.4246,  ..., -1.0977,  2.4766, -1.0039]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 72]\n",
      "skip_mask: False, conf: 0.389892578125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-10.5078, -11.4766,   0.0692,  ...,  -5.1055,  -8.0000,  -5.1406]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.3403, -2.2832,  0.6411,  ...,  1.9600,  5.1406, -3.0059]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 73]\n",
      "skip_mask: False, conf: 0.0234222412109375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-6.7070, -8.6484,  4.1250,  ..., -0.6953, -4.9258, -0.5884]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.3464,  1.0225, -1.6914,  ...,  1.0459, -0.1759, -1.9316]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 74]\n",
      "skip_mask: False, conf: 0.001190185546875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-6.1641, -5.5117,  4.7734,  ..., -2.1914, -3.9199, -1.1191]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 1.2861, -0.1399, -1.9482,  ..., -0.9707,  2.3047,  3.2383]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 75]\n",
      "skip_mask: False, conf: 0.013824462890625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ -9.6875, -10.0312,   1.1934,  ...,  -6.1172,  -7.3867,  -5.0352]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.6494, -2.8984, -0.8535,  ..., -2.0996,  0.7656,  3.0781]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 76]\n",
      "skip_mask: False, conf: 0.0066680908203125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-5.6250, -7.6641,  1.4688,  ..., -2.4297, -4.3867, -2.4629]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.0539, -0.9736, -0.2039,  ...,  1.8652,  1.8398, -0.1520]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 77]\n",
      "skip_mask: False, conf: 0.0023651123046875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.1855, -2.8145,  8.0078,  ...,  2.2559, -0.8525,  2.6855]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.7593, -4.7891,  1.0879,  ..., -1.4902,  1.6787, -4.3047]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 78]\n",
      "skip_mask: False, conf: 0.0179443359375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-6.8203, -6.2383,  2.5508,  ..., -2.8438, -4.9180, -2.5918]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.0820, -6.2617, -0.7397,  ...,  0.0446, -1.3486,  1.0898]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 79]\n",
      "skip_mask: False, conf: 0.0014190673828125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-7.1445, -6.3164,  2.4434,  ..., -4.8164, -6.2344, -4.3125]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.8115, -4.2734,  0.3389,  ..., -1.0010, -3.0430, -1.4180]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 80]\n",
      "skip_mask: False, conf: 0.00099945068359375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-8.7266, -7.7500,  1.0381,  ..., -5.1211, -7.6406, -4.0977]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.9189, -2.8809, -1.0938,  ...,  0.0980,  0.8398, -1.8379]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 81]\n",
      "skip_mask: False, conf: 0.1494140625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.9600, -0.9644, 10.2734,  ...,  2.8398,  0.5508,  1.1406]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.5522,  1.1035, -2.5840,  ..., -0.6187,  1.1846, -0.5205]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 82]\n",
      "skip_mask: False, conf: 0.000152587890625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-5.7227, -3.9551,  6.1562,  ...,  1.2666, -2.9434, -1.9121]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.0762,  2.0938, -1.5898,  ...,  2.2246, -0.2472, -0.7866]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 83]\n",
      "skip_mask: False, conf: 0.0101165771484375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-8.1094, -8.3906,  2.5625,  ..., -2.5684, -4.2891, -2.0020]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.0488, -2.6152, -1.4014,  ...,  2.2148,  0.0340, -1.7266]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 84]\n",
      "skip_mask: False, conf: 0.0128173828125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ -8.5625, -10.0312,   2.4043,  ...,  -3.1641,  -6.0039,  -2.5059]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.3867, -1.2861, -1.1758,  ..., -1.1143,  1.2178, -2.1602]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 85]\n",
      "skip_mask: False, conf: 0.0838623046875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-6.0586, -6.5391,  2.5527,  ..., -2.0508, -3.5605, -1.9678]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.7622,  6.4922, -0.4204,  ..., -0.7236,  3.6582,  3.5703]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 86]\n",
      "skip_mask: False, conf: 0.0001220703125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-5.7461, -4.2539,  2.1777,  ..., -2.7695, -3.8125, -2.7617]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.3398, -1.2979,  0.0212,  ..., -3.2871, -3.9902, -2.4688]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 87]\n",
      "skip_mask: False, conf: 0.7265625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-4.0625, -7.2578,  0.3989,  ..., -1.4717, -5.2617, -2.8262]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.1956,  4.7188, -1.5566,  ..., -1.5625,  2.0586, -1.4521]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 88]\n",
      "skip_mask: False, conf: 0.00113677978515625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ -6.1289, -12.8984,   2.0820,  ...,  -3.7188,  -2.3652,  -3.8086]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.0381, -0.8140,  0.3757,  ..., -4.2891,  3.4688, -2.3691]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 89]\n",
      "skip_mask: False, conf: 0.016632080078125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-3.7637, -6.6992,  3.2637,  ..., -2.4590, -2.2305, -0.7427]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[ 0.3232, -1.6875, -0.5664,  ...,  0.9829, -1.7480,  0.1069]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 90]\n",
      "skip_mask: False, conf: 0.00672149658203125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.5039, -6.3086,  7.5352,  ..., -1.0156, -1.7061, -1.5596]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[0.5469, 0.4844, 0.0190,  ..., 1.0049, 0.9087, 3.8691]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 91]\n",
      "skip_mask: False, conf: 0.0113067626953125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.5669, -0.0453,  8.3672,  ...,  0.1050, -0.1212, -1.4609]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.13s/it, est. speed input: 4.16 toks/s, output: 32.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.4443, -0.4487,  1.0566,  ..., -3.2539,  4.3516, -0.4888]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 92]\n",
      "skip_mask: False, conf: 0.0054473876953125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.8618, -0.2849,  6.8242,  ..., -1.3984,  0.7227,  0.7676]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.0215, -2.9629, -1.2988,  ..., -1.1816,  3.0801,  4.3750]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 93]\n",
      "skip_mask: False, conf: 0.01263427734375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.1289, -1.0166,  7.0000,  ..., -1.3457, -1.5820,  0.6685]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-1.3086, -3.4766,  1.4609,  ..., -4.0938, -0.2913,  0.2365]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 94]\n",
      "skip_mask: False, conf: 0.007537841796875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.8027, -2.3887, 10.3906,  ..., -0.9033, -3.4336, -0.6533]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.4907,  1.4629,  2.9434,  ...,  0.8765,  0.0577, -0.6392]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 95]\n",
      "skip_mask: False, conf: 0.01136016845703125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-3.1367, -0.2781,  7.7500,  ..., -1.3018, -2.6953, -1.1475]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.9097,  1.6982,  0.2468,  ..., -0.1881,  0.3188, -0.5283]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 96]\n",
      "skip_mask: False, conf: 0.0171966552734375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-4.6445, -5.1992,  5.0234,  ..., -0.7744, -2.9355, -6.2969]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-0.9004, -3.2598,  0.3728,  ...,  0.0779, -1.0645, -2.9824]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 97]\n",
      "skip_mask: False, conf: 0.005157470703125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.0352, -7.1250,  4.2578,  ...,  0.8608, -1.8916, -1.2139]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[1.5469, 0.3711, 0.9692,  ..., 1.0801, 1.9580, 0.1835]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [3, 98]\n",
      "skip_mask: False, conf: 0.012176513671875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[compute_logits] logits: tensor([[-2.1406, -4.4766,  7.9453,  ..., -1.9668, -1.9014, -0.6680]],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_batch_inference(llm, \"lmsys_1.jsonl\", \"results/lmsys_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "megatron_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
