{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-21 15:28:21 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams, ModelRegistry\n",
    "#from vllm.model_executor.models.llama_ee import LlamaForCausalLMEE\n",
    "#ModelRegistry.register_model(\"LlamaForCausalLMEE\", LlamaForCausalLMEE)\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-21 15:28:34 config.py:520] This model supports multiple tasks: {'score', 'classify', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 02-21 15:28:34 cuda.py:100] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 02-21 15:28:34 config.py:656] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 02-21 15:28:34 llm_engine.py:232] Initializing an LLM engine (v0.7.0) with config: model='meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-2-7b-chat-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 02-21 15:28:36 cuda.py:174] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 02-21 15:28:36 cuda.py:222] Using XFormers backend.\n",
      "INFO 02-21 15:28:37 model_runner.py:1110] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-21 15:28:38 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be20f61e1d948b4b5e2153e8e6d78dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-21 15:28:44 model_runner.py:1115] Loading model weights took 12.5533 GB\n",
      "input_ids shape before converting: torch.Size([2048])\n",
      "input_ids shape after converting: torch.Size([1, 2048])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 2048])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 2048])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 1]\n",
      "skip_mask: False, conf: 0.01666259765625\n",
      "--------EE statistics---------\n",
      "INFO 02-21 15:28:45 worker.py:266] Memory profiling takes 1.07 seconds\n",
      "INFO 02-21 15:28:45 worker.py:266] the current vLLM instance can use total_gpu_memory (15.77GiB) x gpu_memory_utilization (0.90) = 14.19GiB\n",
      "INFO 02-21 15:28:45 worker.py:266] model weights take 12.55GiB; non_torch_memory takes 0.07GiB; PyTorch activation peak memory takes 0.79GiB; the rest of the memory reserved for KV Cache is 0.78GiB.\n",
      "INFO 02-21 15:28:46 executor_base.py:108] # CUDA blocks: 99, # CPU blocks: 512\n",
      "INFO 02-21 15:28:46 executor_base.py:113] Maximum concurrency for 1024 tokens per request: 1.55x\n",
      "INFO 02-21 15:28:48 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 4.30 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"meta-llama/Llama-2-7b-chat-hf\", max_model_len=1024, enforce_eager=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-21 01:05:07 config.py:520] This model supports multiple tasks: {'embed', 'generate', 'score', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 02-21 01:05:07 llm_engine.py:232] Initializing an LLM engine (v0.7.0) with config: model='meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-2-7b-chat-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 02-21 01:05:08 cuda.py:174] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 02-21 01:05:08 cuda.py:222] Using XFormers backend.\n",
      "INFO 02-21 01:05:10 model_runner.py:1110] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "Initialized LlamaForCausalLM with prefix \n",
      "INFO 02-21 01:05:10 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fb1a005f3e461b8d06aab4655fca54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-21 01:05:16 model_runner.py:1115] Loading model weights took 12.5523 GB\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2048])\n",
      "[compute_logits] Hidden states shape: torch.Size([2048, 4096])\n",
      "INFO 02-21 01:05:17 worker.py:266] Memory profiling takes 0.86 seconds\n",
      "INFO 02-21 01:05:17 worker.py:266] the current vLLM instance can use total_gpu_memory (15.77GiB) x gpu_memory_utilization (0.90) = 14.19GiB\n",
      "INFO 02-21 01:05:17 worker.py:266] model weights take 12.55GiB; non_torch_memory takes 0.07GiB; PyTorch activation peak memory takes 0.31GiB; the rest of the memory reserved for KV Cache is 1.25GiB.\n",
      "INFO 02-21 01:05:17 executor_base.py:108] # CUDA blocks: 160, # CPU blocks: 512\n",
      "INFO 02-21 01:05:17 executor_base.py:113] Maximum concurrency for 1024 tokens per request: 2.50x\n",
      "INFO 02-21 01:05:20 model_runner.py:1430] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([256])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([256])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   3%|â–Ž         | 1/35 [00:00<00:21,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([248])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([248])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([248])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   6%|â–Œ         | 2/35 [00:01<00:19,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([240])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([240])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([240])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   9%|â–Š         | 3/35 [00:01<00:18,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([232])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([232])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([232])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  11%|â–ˆâ–        | 4/35 [00:02<00:17,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([224])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([224])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  14%|â–ˆâ–        | 5/35 [00:02<00:16,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([216])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([216])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([216])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  17%|â–ˆâ–‹        | 6/35 [00:03<00:15,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([208])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([208])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([208])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  20%|â–ˆâ–ˆ        | 7/35 [00:03<00:15,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([200])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([200])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([200])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:04<00:14,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([192])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([192])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:04<00:14,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([184])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([184])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([184])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:05<00:13,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([176])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([176])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([176])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [00:06<00:13,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([168])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([168])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([168])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:06<00:12,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([160])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([160])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([160])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:07<00:11,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([152])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([152])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([152])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:07<00:11,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([144])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([144])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([144])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:08<00:11,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([136])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([136])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([136])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:08<00:10,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([128])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([128])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:09<00:09,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([120])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([120])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([120])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:09<00:09,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([112])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([112])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([112])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:10<00:08,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([104])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([104])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([104])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:10<00:07,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([96])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([96])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([96])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:11<00:07,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([88])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([88])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([88])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [00:12<00:07,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([80])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([80])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([80])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23/35 [00:12<00:06,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([72])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([72])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([72])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:13<00:06,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([64])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([64])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25/35 [00:13<00:05,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([56])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([56])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([56])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [00:14<00:04,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([48])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([48])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([48])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 27/35 [00:14<00:04,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([40])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([40])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:15<00:03,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([32])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([32])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 29/35 [00:15<00:03,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([24])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([24])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([24])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [00:16<00:02,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([16])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([16])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [00:16<00:02,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([8])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([8])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:17<00:01,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([4])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([4])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/35 [00:17<00:01,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:18<00:00,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:18<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-21 01:05:38 model_runner.py:1558] Graph capturing finished in 19 secs, took 0.24 GiB\n",
      "INFO 02-21 01:05:38 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 22.70 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Below is the output of running the original llama code (llama_original.py) for reference\n",
    "llm = LLM(model=\"meta-llama/Llama-2-7b-chat-hf\", max_model_len=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  8.39it/s, est. speed input: 56.62 toks/s, output: 134.20 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Hello, my name is', Generated text: \" Sherry and I'm a 35-year-old woman from\"\n",
      "Prompt: 'The president of the United States is', Generated text: ' a member of Congress, which means that he or she is subject to the same'\n",
      "Prompt: 'The capital of France is', Generated text: ' Paris. This is a fact that is well known and widely accepted. However,'\n",
      "Prompt: 'The future of AI is', Generated text: ' likely to be shaped by a combination of technological advancements, soci'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Output of original lammama for reference\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape before converting: torch.Size([27])\n",
      "input_ids shape after converting: torch.Size([1, 27])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 27])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 27])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 2]\n",
      "skip_mask: False, conf: 0.0088043212890625\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 3]\n",
      "skip_mask: False, conf: 0.1258544921875\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 4]\n",
      "skip_mask: False, conf: 0.014495849609375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 5]\n",
      "skip_mask: False, conf: 0.00244140625\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 6]\n",
      "skip_mask: False, conf: 0.84619140625\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 7]\n",
      "skip_mask: False, conf: 0.591796875\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [1, 7]\n",
      "skip_mask: True, conf: 0.9990234375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [1, 8]\n",
      "skip_mask: False, conf: 0.3779296875\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 8]\n",
      "skip_mask: True, conf: 0.98876953125\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 9]\n",
      "skip_mask: False, conf: 0.8046875\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 10]\n",
      "skip_mask: False, conf: 0.2178955078125\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  4.47it/s, est. speed input: 30.20 toks/s, output: 71.59 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 11]\n",
      "skip_mask: False, conf: 0.210693359375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 12]\n",
      "skip_mask: False, conf: 0.394287109375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 13]\n",
      "skip_mask: False, conf: 0.008636474609375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 14]\n",
      "skip_mask: False, conf: 0.021636962890625\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 15]\n",
      "skip_mask: False, conf: 0.160888671875\n",
      "--------EE statistics---------\n",
      "Prompt: 'Hello, my name is', Generated text: \" Sherry and I'm addicted addicted to reading. I know,\"\n",
      "Prompt: 'The president of the United States is', Generated text: ' a member of Congress, which means that they are an elected official who serves in'\n",
      "Prompt: 'The capital of France is', Generated text: ' Paris. This is a factoid that many people know and can easily verify.'\n",
      "Prompt: 'The future of AI is', Generated text: ' likely to be shaped by factors such as advances in computing power, data'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# output of llama-ee (current llama.py is llama-ee)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "megatron_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
