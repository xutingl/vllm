{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-22 20:20:52 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams, ModelRegistry\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "!export CUDA_VISIBLE_DEVICES=1\n",
    "#from vllm.model_executor.models.llama_ee import LlamaForCausalLMEE\n",
    "#ModelRegistry.register_model(\"LlamaForCausalLMEE\", LlamaForCausalLMEE)\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_inference(llm, input_file, output_file, batch_size=8):\n",
    "    prompts = []\n",
    "    with open(input_file, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            prompts.append(json.loads(line)[\"text\"])\n",
    "\n",
    "    sampling_params = SamplingParams(seed=42, max_tokens=100)\n",
    "\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "    input_prompt = []\n",
    "    generated_text = []\n",
    "    arrival_time = []\n",
    "    last_token_time = []\n",
    "    first_scheduled_time = []\n",
    "    first_token_time = []\n",
    "    time_in_queue = []\n",
    "    finished_time = []\n",
    "    scheduler_time = []\n",
    "    model_forward_time = []\n",
    "    model_execute_time = []\n",
    "    for output in outputs:\n",
    "        input_prompt.append(output.prompt)\n",
    "        generated_text.append(output.outputs[0].text)\n",
    "        metrics = output.metrics\n",
    "        arrival_time.append(metrics.arrival_time)\n",
    "        last_token_time.append(metrics.last_token_time)\n",
    "        first_scheduled_time.append(metrics.first_scheduled_time)\n",
    "        first_token_time.append(metrics.first_token_time)\n",
    "        time_in_queue.append(metrics.time_in_queue)\n",
    "        finished_time.append(metrics.finished_time)\n",
    "        scheduler_time.append(metrics.scheduler_time)\n",
    "        model_forward_time.append(metrics.model_forward_time)\n",
    "        model_execute_time.append(metrics.model_execute_time)\n",
    "    df = pd.DataFrame({\n",
    "        \"input_prompt\": input_prompt,\n",
    "        \"generated_text\": generated_text,\n",
    "        \"arrival_time\": arrival_time,\n",
    "        \"last_token_time\": last_token_time,\n",
    "        \"first_scheduled_time\": first_scheduled_time,\n",
    "        \"first_token_time\": first_token_time,\n",
    "        \"time_in_queue\": time_in_queue,\n",
    "        \"finished_time\": finished_time,\n",
    "        \"scheduler_time\": scheduler_time,\n",
    "        \"model_forward_time\": model_forward_time,\n",
    "        \"model_execute_time\": model_execute_time\n",
    "    })\n",
    "    df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-22 20:21:06 config.py:520] This model supports multiple tasks: {'score', 'generate', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 03-22 20:21:06 cuda.py:100] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 03-22 20:21:06 config.py:656] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 03-22 20:21:06 llm_engine.py:232] Initializing an LLM engine (v0.7.0) with config: model='meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-2-7b-chat-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 03-22 20:21:08 cuda.py:174] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-22 20:21:08 cuda.py:222] Using XFormers backend.\n",
      "INFO 03-22 20:21:09 model_runner.py:1110] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "INFO 03-22 20:21:10 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9bce3a90a04bb78afd0c98926f2d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-22 20:21:15 model_runner.py:1115] Loading model weights took 12.5533 GB\n",
      "input_ids shape before converting: torch.Size([2048])\n",
      "input_ids shape after converting: torch.Size([2048])\n",
      "LlamaModel forward. input_ids shape: torch.Size([2048])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([2048])\n",
      "[compute_logits] hidden_states shape: torch.Size([2048, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 1]\n",
      "skip_mask: False, conf: 0.0\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([2048, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2048, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([2048, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "INFO 03-22 20:21:16 worker.py:266] Memory profiling takes 1.02 seconds\n",
      "INFO 03-22 20:21:16 worker.py:266] the current vLLM instance can use total_gpu_memory (15.77GiB) x gpu_memory_utilization (0.90) = 14.19GiB\n",
      "INFO 03-22 20:21:16 worker.py:266] model weights take 12.55GiB; non_torch_memory takes 0.07GiB; PyTorch activation peak memory takes 0.32GiB; the rest of the memory reserved for KV Cache is 1.25GiB.\n",
      "INFO 03-22 20:21:16 executor_base.py:108] # CUDA blocks: 159, # CPU blocks: 512\n",
      "INFO 03-22 20:21:16 executor_base.py:113] Maximum concurrency for 1024 tokens per request: 2.48x\n",
      "INFO 03-22 20:21:19 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 4.11 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"meta-llama/Llama-2-7b-chat-hf\", max_model_len=1024, enforce_eager=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-22 18:58:53 config.py:520] This model supports multiple tasks: {'generate', 'classify', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 03-22 18:58:53 cuda.py:100] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 03-22 18:58:53 config.py:656] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 03-22 18:58:53 llm_engine.py:232] Initializing an LLM engine (v0.7.0) with config: model='meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-2-7b-chat-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 03-22 18:58:55 cuda.py:174] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-22 18:58:55 cuda.py:222] Using XFormers backend.\n",
      "INFO 03-22 18:58:57 model_runner.py:1110] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "Initialized LlamaForCausalLM with prefix \n",
      "INFO 03-22 18:58:57 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c31f579fec74e259f30a22fa3041182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-22 18:59:02 model_runner.py:1115] Loading model weights took 12.5523 GB\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([2048])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([2048])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([2048, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([2048, 4096])\n",
      "INFO 03-22 18:59:03 worker.py:266] Memory profiling takes 0.84 seconds\n",
      "INFO 03-22 18:59:03 worker.py:266] the current vLLM instance can use total_gpu_memory (15.77GiB) x gpu_memory_utilization (0.90) = 14.19GiB\n",
      "INFO 03-22 18:59:03 worker.py:266] model weights take 12.55GiB; non_torch_memory takes 0.07GiB; PyTorch activation peak memory takes 0.31GiB; the rest of the memory reserved for KV Cache is 1.25GiB.\n",
      "INFO 03-22 18:59:03 executor_base.py:108] # CUDA blocks: 160, # CPU blocks: 512\n",
      "INFO 03-22 18:59:03 executor_base.py:113] Maximum concurrency for 1024 tokens per request: 2.50x\n",
      "INFO 03-22 18:59:06 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 3.92 seconds\n"
     ]
    }
   ],
   "source": [
    "# Below is the output of running the original llama code (llama_original.py) for reference\n",
    "original_llm = LLM(model=\"meta-llama/Llama-2-7b-chat-hf\", max_model_len=1024, enforce_eager=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4/4 [00:00<00:00,  8.39it/s, est. speed input: 56.62 toks/s, output: 134.20 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Hello, my name is', Generated text: \" Sherry and I'm a 35-year-old woman from\"\n",
      "Prompt: 'The president of the United States is', Generated text: ' a member of Congress, which means that he or she is subject to the same'\n",
      "Prompt: 'The capital of France is', Generated text: ' Paris. This is a fact that is well known and widely accepted. However,'\n",
      "Prompt: 'The future of AI is', Generated text: ' likely to be shaped by a combination of technological advancements, soci'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Output of original lammama for reference\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape before converting: torch.Size([27])\n",
      "input_ids shape after converting: torch.Size([1, 27])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 27])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 27])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 2]\n",
      "skip_mask: False, conf: 0.0088043212890625\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 3]\n",
      "skip_mask: False, conf: 0.1258544921875\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 4]\n",
      "skip_mask: False, conf: 0.014495849609375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 5]\n",
      "skip_mask: False, conf: 0.00244140625\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 6]\n",
      "skip_mask: False, conf: 0.84619140625\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [0, 7]\n",
      "skip_mask: False, conf: 0.591796875\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [1, 7]\n",
      "skip_mask: True, conf: 0.9990234375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [1, 8]\n",
      "skip_mask: False, conf: 0.3779296875\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 8]\n",
      "skip_mask: True, conf: 0.98876953125\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 9]\n",
      "skip_mask: False, conf: 0.8046875\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 10]\n",
      "skip_mask: False, conf: 0.2178955078125\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4/4 [00:00<00:00,  4.47it/s, est. speed input: 30.20 toks/s, output: 71.59 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 11]\n",
      "skip_mask: False, conf: 0.210693359375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 12]\n",
      "skip_mask: False, conf: 0.394287109375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 13]\n",
      "skip_mask: False, conf: 0.008636474609375\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 14]\n",
      "skip_mask: False, conf: 0.021636962890625\n",
      "--------EE statistics---------\n",
      "input_ids shape before converting: torch.Size([4])\n",
      "input_ids shape after converting: torch.Size([1, 4])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1, 4])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1, 4])\n",
      "Force kv_caches to None\n",
      "--------EE statistics---------\n",
      "self.exited_rates: [2, 15]\n",
      "skip_mask: False, conf: 0.160888671875\n",
      "--------EE statistics---------\n",
      "Prompt: 'Hello, my name is', Generated text: \" Sherry and I'm addicted addicted to reading. I know,\"\n",
      "Prompt: 'The president of the United States is', Generated text: ' a member of Congress, which means that they are an elected official who serves in'\n",
      "Prompt: 'The capital of France is', Generated text: ' Paris. This is a factoid that many people know and can easily verify.'\n",
      "Prompt: 'The future of AI is', Generated text: ' likely to be shaped by factors such as advances in computing power, data'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# output of llama-ee (current llama.py is llama-ee)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([13])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([13])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([13, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([13, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.32s/it, est. speed input: 5.60 toks/s, output: 43.05 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaModel forward] Input_ids shape: torch.Size([1])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] Hidden states shape: torch.Size([1, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_batch_inference(original_llm, \"lmsys_1.jsonl\", \"results/lmsys_1_original.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape before converting: torch.Size([13])\n",
      "input_ids shape after converting: torch.Size([13])\n",
      "LlamaModel forward. input_ids shape: torch.Size([13])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([13])\n",
      "[compute_logits] hidden_states shape: torch.Size([13, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "[get_skip_mask] mask dim: 1\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 2]\n",
      "skip_mask: False, conf: 0.0063629150390625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([13, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([13, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([13, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 3]\n",
      "skip_mask: False, conf: 0.0044403076171875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 4]\n",
      "skip_mask: False, conf: 0.002197265625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 5]\n",
      "skip_mask: False, conf: 0.031494140625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 6]\n",
      "skip_mask: False, conf: 0.01898193359375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 7]\n",
      "skip_mask: False, conf: 0.012451171875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 8]\n",
      "skip_mask: False, conf: 0.0072784423828125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 9]\n",
      "skip_mask: False, conf: 0.00274658203125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 10]\n",
      "skip_mask: False, conf: 0.00066375732421875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 11]\n",
      "skip_mask: False, conf: 0.396728515625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 12]\n",
      "skip_mask: False, conf: 0.189697265625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 13]\n",
      "skip_mask: False, conf: 0.0007476806640625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 14]\n",
      "skip_mask: False, conf: 0.028839111328125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 15]\n",
      "skip_mask: False, conf: 0.0112762451171875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 16]\n",
      "skip_mask: False, conf: 0.009185791015625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 17]\n",
      "skip_mask: False, conf: 0.0164031982421875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 18]\n",
      "skip_mask: False, conf: 0.00315093994140625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 19]\n",
      "skip_mask: False, conf: 0.016754150390625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 20]\n",
      "skip_mask: False, conf: 0.00396728515625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 21]\n",
      "skip_mask: False, conf: 0.00124359130859375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 22]\n",
      "skip_mask: False, conf: 0.001190185546875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 23]\n",
      "skip_mask: False, conf: 0.00347137451171875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 24]\n",
      "skip_mask: False, conf: 0.0062713623046875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 25]\n",
      "skip_mask: False, conf: 0.2056884765625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 26]\n",
      "skip_mask: False, conf: 0.00734710693359375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 27]\n",
      "skip_mask: False, conf: 0.009063720703125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 28]\n",
      "skip_mask: False, conf: 0.0133209228515625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 29]\n",
      "skip_mask: False, conf: 0.00101470947265625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 30]\n",
      "skip_mask: False, conf: 0.003376007080078125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 31]\n",
      "skip_mask: False, conf: 0.0126495361328125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 32]\n",
      "skip_mask: False, conf: 0.03985595703125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 33]\n",
      "skip_mask: False, conf: 0.0072479248046875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 34]\n",
      "skip_mask: False, conf: 6.866455078125e-05\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 35]\n",
      "skip_mask: False, conf: 0.00714111328125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 36]\n",
      "skip_mask: False, conf: 0.0146636962890625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 37]\n",
      "skip_mask: False, conf: 0.050140380859375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 38]\n",
      "skip_mask: False, conf: 0.034576416015625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 39]\n",
      "skip_mask: False, conf: 0.0001220703125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 40]\n",
      "skip_mask: False, conf: 0.0072479248046875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 41]\n",
      "skip_mask: False, conf: 0.00238037109375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 42]\n",
      "skip_mask: False, conf: 0.0236968994140625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 43]\n",
      "skip_mask: False, conf: 0.017791748046875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 44]\n",
      "skip_mask: False, conf: 0.00493621826171875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 45]\n",
      "skip_mask: False, conf: 0.00457000732421875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 46]\n",
      "skip_mask: False, conf: 0.016876220703125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 47]\n",
      "skip_mask: False, conf: 0.0235443115234375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 48]\n",
      "skip_mask: False, conf: 0.0012969970703125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 49]\n",
      "skip_mask: False, conf: 0.0036163330078125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 50]\n",
      "skip_mask: False, conf: 0.004302978515625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 51]\n",
      "skip_mask: False, conf: 0.00232696533203125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 52]\n",
      "skip_mask: False, conf: 0.0489501953125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 53]\n",
      "skip_mask: False, conf: 0.00238037109375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 54]\n",
      "skip_mask: False, conf: 0.000518798828125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 55]\n",
      "skip_mask: False, conf: 0.003154754638671875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 56]\n",
      "skip_mask: False, conf: 0.0052947998046875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 57]\n",
      "skip_mask: False, conf: 0.01263427734375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 58]\n",
      "skip_mask: False, conf: 0.001171112060546875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 59]\n",
      "skip_mask: False, conf: 0.0048980712890625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 60]\n",
      "skip_mask: False, conf: 0.00485992431640625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 61]\n",
      "skip_mask: False, conf: 0.01971435546875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 62]\n",
      "skip_mask: False, conf: 0.0026397705078125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 63]\n",
      "skip_mask: False, conf: 0.01390838623046875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 64]\n",
      "skip_mask: False, conf: 0.00440216064453125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 65]\n",
      "skip_mask: False, conf: 0.003749847412109375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 66]\n",
      "skip_mask: False, conf: 0.03125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 67]\n",
      "skip_mask: False, conf: 0.01861572265625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 68]\n",
      "skip_mask: False, conf: 0.0022125244140625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 69]\n",
      "skip_mask: False, conf: 0.00591278076171875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 70]\n",
      "skip_mask: False, conf: 0.0153961181640625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 71]\n",
      "skip_mask: False, conf: 0.00647735595703125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 72]\n",
      "skip_mask: False, conf: 0.01312255859375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 73]\n",
      "skip_mask: False, conf: 0.015899658203125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 74]\n",
      "skip_mask: False, conf: 0.00673675537109375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 75]\n",
      "skip_mask: False, conf: 0.0058135986328125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 76]\n",
      "skip_mask: False, conf: 0.0098114013671875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 77]\n",
      "skip_mask: False, conf: 0.00353240966796875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 78]\n",
      "skip_mask: False, conf: 0.007171630859375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 79]\n",
      "skip_mask: False, conf: 0.0582275390625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 80]\n",
      "skip_mask: False, conf: 0.037567138671875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 81]\n",
      "skip_mask: False, conf: 0.0128173828125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 82]\n",
      "skip_mask: False, conf: 0.00030517578125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 83]\n",
      "skip_mask: False, conf: 0.003082275390625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 84]\n",
      "skip_mask: False, conf: 0.010589599609375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 85]\n",
      "skip_mask: False, conf: 0.0112762451171875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 86]\n",
      "skip_mask: False, conf: 0.77978515625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 87]\n",
      "skip_mask: False, conf: 0.0074005126953125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 88]\n",
      "skip_mask: False, conf: 0.022918701171875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 89]\n",
      "skip_mask: False, conf: 0.00433349609375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 90]\n",
      "skip_mask: False, conf: 0.040771484375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 91]\n",
      "skip_mask: False, conf: 0.0011138916015625\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 92]\n",
      "skip_mask: False, conf: 0.00629425048828125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 93]\n",
      "skip_mask: False, conf: 0.00927734375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 94]\n",
      "skip_mask: False, conf: 0.025360107421875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 95]\n",
      "skip_mask: False, conf: 0.007049560546875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 96]\n",
      "skip_mask: False, conf: 0.0233001708984375\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.84s/it, est. speed input: 3.39 toks/s, output: 26.08 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 97]\n",
      "skip_mask: False, conf: 0.0057373046875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 98]\n",
      "skip_mask: False, conf: 0.0293731689453125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 99]\n",
      "skip_mask: False, conf: 0.04180908203125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 100]\n",
      "skip_mask: False, conf: 0.00055694580078125\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "input_ids shape before converting: torch.Size([1])\n",
      "input_ids shape after converting: torch.Size([1])\n",
      "LlamaModel forward. input_ids shape: torch.Size([1])\n",
      "attention_mask shape: None\n",
      "-------------------------\n",
      "Is first rank. input ids shape: torch.Size([1])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n",
      "--------[LlamaModel: forward] EE statistics at layer20---------\n",
      "self.exited_rates([num_ee, num_no_ee]): [0, 101]\n",
      "skip_mask: False, conf: 0.000438690185546875\n",
      "--------[LlamaModel: forward] EE statistics---------\n",
      "[LlamaModel: forward] Returning hidden_states shape: torch.Size([1, 4096])\n",
      "[LlamaForCausalLM forward] Model output shape: torch.Size([1, 4096])\n",
      "[compute_logits] hidden_states shape: torch.Size([1, 4096]). lm_head shape: ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_batch_inference(llm, \"lmsys_1.jsonl\", \"results/lmsys_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "megatron_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
